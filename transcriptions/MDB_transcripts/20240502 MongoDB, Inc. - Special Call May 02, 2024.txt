MongoDB, Inc. - Special Call
Event Details
May 02, 2024
3:00 PM GMT+1
Corporate Participants
Michael Lawrence Gordon MongoDB, Inc. · COO & CFO
Andrew Davidson
Paul Done
Sahir Azam MongoDB, Inc. · Chief Product Officer
Dev C. Ittycheria MongoDB, Inc. · President, CEO & Director
Conference Call Participants
Brandon Duderstadt
Lin Qiao Fireworks.ai, Inc. · CEO, CFO & Secretary
Junkai Liu LlamaIndex Inc. · President
Brian Hanks
Ha Hoang
Hjalmar Van Raemdonck
Kevin O'Dell
Yelena Shtykel
Alvaro Celis
Unknown Analyst
Sanjit Kumar Singh Morgan Stanley, Research Division · Vice President
Raimo Lenschow Barclays Bank PLC, Research Division · MD & Analyst
Brad Robert Reback Stifel, Nicolaus & Company, Incorporated, Research Division · MD & Senior Equity Research Analyst
Rishi Nitya Jaluria RBC Capital Markets, Research Division · Analyst
Brent Alan Bracelin Piper Sandler & Co., Research Division · MD & Senior Research Analyst
Michael Joseph Cikos Needham & Company, LLC, Research Division · Senior Analyst
Jason Noah Ader William Blair & Company L.L.C., Research Division · Partner & Co-Group Head of Technology, Media and Communications
Event Transcript
Revised May 06, 2024


Prepared Remarks
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
All righty. Thank you all for coming. I hope you're able to enjoy and join the keynotes, a bunch of great announcements. We've got a pretty packed agenda here. So I'll try and help us get right into it. We're going to start off with a product update. As I think people know, we mostly do these to give some pretty deep insight into our products, our product road map, give you access and insight into our customers, how they're using our product because obviously, we tell you a lot of things, but we always find it really helpful in this format and this venue to give you as much exposure to that side of the house. So that's really what dominates the bulk of the agenda.
We'll have an AI partner panel just because of the recent announcements there and how important a topic that is. That should be interesting. We'll host a customer panel to, again, sort of give you that customer view of how they're using MongoDB and what's going on, talk about partners. I'll spend a minute giving kind of just a quick business update and then we'll have Q&A with Dave and me and Sahir.
So with that, let's dive in. Obviously, I provide all the appropriate safe harbor statements. But I'll actually particularly underscore this because the timing of this event is a little bit unusual. Our quarter just ended 2 days ago for those who don't know. And so -- but the Javits Center was available today. And so we have this sort of unusual timing. So obviously, we're not going to talk about any recent events or trends or anything like that, and you'll just save us a lot of household headache if you don't ask about it. So with that, let's dive right in.
So first, we're pursuing one of the largest markets in all of software. You've heard us say this before. The market is $94 billion in 2024 per IDC. That's growing to $153 billion in 2028. That's almost $60 billion of market creation, right? So when we talk about market size, that gives you a little bit of a sense of what we're talking about. It's growing quite quickly and quite rapidly. And that's because it sits at sort of the center of the strategic nexus for what helps businesses drive competitive advantage from their technology, right? And that's the position that we're in.
There are also some differences in our market though. Many markets in software tend to be driven by a unit of competition that is the customer, right? The results in those accounts are binary. We at MongoDB use Workday or we do not use Workday. Parts of the team, parts of the organization don't use different HR software than others, right? It's sort of somewhat monolithic within the application, within the state. But in our market, it's quite different. What you see is the unit of competition is the workload.
So that means that, in general, we compete one workload at a time. We're never fully shut out of account. There are always opportunities given that people are constantly creating new applications. But just because you win an application within a big bank doesn't suddenly mean that they take all of their hundreds of thousands of applications and migrate them to MongoDB. So it's an important dynamic in our market, it's different than what exists in other markets and it's one of the things we want to call out.
And therefore, to grow with an account, we typically need to acquire more workloads over time. Yes, an individual application will grow over time, but to really see the kind of growth that we're seeing in to kind of capture our full market potential, you need to win more applications within the account.
So we're going to spend a lot of time over the course of these couple of hours talking about that. And a lot of what you heard in the product announcements and from us over the quarters, is focused on how do we win more workloads within an account. And you can see that this sort of shows a progression over time. The columns here represent different business lines. The rows with effectively different applications. It's really more illustrative obviously, but just shows that you try and penetrate and account more quickly and more deeply over time.
The best way to win workloads more quickly within an account is to become a standard within the account. And there isn't one sort of textbook Merriam-Webster dictionary definition of what being a standard is. But in general, for us, what becoming a standard means is that we're approved to be broadly what we used within the account. We're going to talk a bunch about this. This is really what our efforts have all been oriented around. Once we become a standard, there are a bunch of different benefits to us. I'll talk about three.
There's sort of explicit C-level support and endorsement. There's access, really reduced friction within the account and streamline application deployment and support. So once we become a standard within the account, we've got real buy-in from the executive team. They understand the competitive advantage that they're going to help drive from their internally developed software when they leverage MongoDB, whether that's the scalability, the nimbleness, the agility that, that gives us. They see the benefits in developer productivity and performance and they want to leverage that more for their teams.
Secondly, we get open access to developers within the account. They're encouraged to try the platform, to learn more about it if they don't know about it. And all of these things are designed to reduce the friction that complements kind of a bottoms-up motion in terms of going to market and kind of give us greater permission within the account.
And then finally, once you build a workload and once you launch it, put it in production, you've got support, right? Centralized support often from the IT team, helping you develop, orchestrate the deployments and giving you the peace of mind that you're not left as an island under your own as a company or as a developer team within the company, but you've got the full support of the company behind you.
And so I'm going to walk through three different examples of what becoming a standard looks like it means. These are obviously illustrative, but it gives you a sense for what we're trying to do in our accounts and kind of change the nature of the game from winning workload by workload over time and how do we kind of do that more quickly and more effectively.
So this first chart is a leading global financial services company, a Fortune 100 global services company, top 5 global financial institution in the world. And they had picked MongoDB a number of years ago to solve a critical pain point. That's pretty typical in an account like this where there was some use case where they were really struggling and we were able to help them solve that. But we weren't widely deployed in the account.
As you can see, we kind of grew our ARR over time, adding workloads into account. But these different teams operated independently, each had to manage its own MongoDB instance, but the -- to the institutions credit, they recognized that they needed more enhanced capabilities. They saw the popularity of MongoDB within the account. And so they started narrowing in on MongoDB Atlas, particularly in this case, on AWS. And they were compelled and drawn by our capabilities on full-tech search, time series, workload isolation and things like that.
And so the momentum starts building in the account. You can see the ARR growing as it goes from 2020 to 2022. Interest is sort of increasing across the developer teams. The leadership is realizing the value of the document model is seeing how popular we are with developers within the company, and it's also drawn to the resiliency of the cloud infrastructure. And so they're very focused on application modernization. They're focused on cloud adoption. And so with those priorities, they realize that MongoDB is the ideal platform for them given the breadth of capabilities, the popularity with all Mongo developers and the seamlessness with which we integrate with the major cloud providers. So all that culminates, as you can see here, at the beginning of January -- beginning of 2023, when they select MongoDB Atlas and make MongoDB Atlas generally available within the institution, right?
So it really signals the sort of key pivotal moment in our partnership with them. And what we're seeing today is we're witnessing product teams and product owners consolidate their technology stack on to MongoDB, replacing Dynamo, replacing Cassandra, replacing MySQL, replacing Sybase as they leverage this unified developer data platform that we have. Also, financial services, as many of you know, highly regulated industry, right? And so there's clear demand for a solution like MongoDB that is multi-cloud, highly resilient and serves all the different needs from a security and other standpoint of those regulated industries.
Secondly, this is a global Fortune 100 healthcare company. So for years, this health insurance company had relied on MongoDB again, for kind of a core critical application. That's how we started out. We started out with use cases around fraud claims and analytics and provider search internally that have become real pain points. And so we won those, and it's great. But they hadn't identified any opportunities to expand significantly, particularly with Atlas.
So we developed a business case working with them, identified the first potential Atlas workload. As they started getting inside it and building kind of the business case, they identified other workloads that sort of made sense for MongoDB.
And ultimately, what we moved into was production readiness testing. We performed incredibly well. They started developing more confidence. They run a process internally where they rank providers. So we went through that internal approval ranking process and that led to them deciding to standardize on MongoDB. And so what you can see is for the past 1.5 years or so, Atlas has been the preferred database within this company and any developer in any business unit can get started with a simple click of a button, right? So when I think about not just the C-suite buying, but that removing a friction, right?
Now I can do it. I can adopt it seamlessly, super easily, click of a button compared to if you're a non-preferred database, you have to go build a business case, get exceptions, et cetera, et cetera. So it gives you a little bit of a sense for like what this means we talk about being a standard and how it plays out in terms of specific companies.
Third, I'll talk about a global retailer. We landed as one of the largest global retailers. We initially won an e-commerce application, free e-commerce delivery for them. And it was pretty standard. We're competing against Dynamo and RDS and Postgres. And again, that application was going fine. And as you can see, applications tend to grow.
But as I said before, to grow with any count, you need to continue to win new applications. So [indiscernible] 2 years later, given the success of that first application that we were selected for an Oracle replacement of their real-time store inventory system. And then what you can see is the customer really saw how valuable Atlas was allowing them to scale across thousands of scores, adding new features quickly, scaling, innovating, providing that agility and competitive advantage that they were looking for.
So in 2021, Atlas was chosen as the modern standard for their R&D organization and across all the kind of key inventory systems. And so today, what the customer has done is they've streamlined internal development. So any team can now create and link their Atlas projects. What they've done is they bought sort of the centralized pool, so they removed that internal bureaucracy, budget allocation, how does this work. So it's just with a simple click. And that's accepted accelerated adoption within those accounts. And we've had recent wins where Atlas Search was chosen over Elastic for the inventory search, Atlas Device Sync was chosen over a homegrown solution, and so we've continued to grow in the account.
So the idea here is really just to help people give a little bit of flavor behind when we say what does it mean to be a standard and what does that look like and what to play out. Again, all these are illustrative and certainly, in many of our accounts, we're not a standard yet, but we're continuing to be a standard in more and more of our accounts over time.
What do you need to be a standard? We need to solve a broad range of solutions. So when you hear us talk about the breadth of use cases and the expansiveness of the developer data platform vision, that's what we're talking about. That's what gives you that potential to give teams confidence, executive teams in particular confidence to build as a platform.
Second, if we accelerate application modernization and help move relational migrations onto our platforms, that's a critical thing that helps strengthen the position and gives customers incremental confidence to invest in us.
And then finally, you heard a lot of talk about AI. Obviously, it's newer over the last year, 1.5 years as a key focal point within development teams, but establishing ourselves as a critical partner for AI is really one of the key things that we do. And again, this also speaks not just to the day-to-day applications and use cases, but also really gives us C-suite level executives the confidence to standardize in MongoDB.
So with that, I am going to -- go backwards 1 second, turn it over to Andrew Davidson, who's our SVP of Product. Thank you, Andrew?
Andrew Davidson
Folks good morning. Good to be here. I'm Andrew Davidson. So yes, I'm going to talk about why solving a broad range of problems is a crucial way for us to accelerate the customer value realization that allows them to make us a standard to consolidate workloads onto our platform. And if we think about what's happened over the last decade in our industry, enterprises have been struggling with running into the fundamental limitations of legacy relational databases and layering in a variety of niche point solutions to work around those limitations.
Whether it's running into scalability challenges, continuous availability challenges that cause someone to layer in a key value store or query challenges that cause someone to layer in a search engine or basically applications that start getting sluggish as they combine more engines so they layer into cash. There's many different shapes of this kind of spaghetti snowflake architectures that you start seeing out there.
And every one of our enterprise customers in this entire industry has some of this going on. This exists on-prem in the legacy environment. It exists in the lift and shift to state in the public cloud, and it exists in the sprawling out-of-control complexity of sprawling cloud services as well. And this complexity has a bunch of problems. It leads to developer friction, lots of different interfaces to manage. It leads to cost overruns because there are so many different components to manage and pay for a lot of duplication of effort. But the governance tax that this puts on your system, it really slows things down, it makes things extremely challenging to move forward. So executives feel that they're investing an enormous amount just to stay afloat because of this complexity.
They can't unlock the business value they're trying to unlock. And developers are spending the vast majority of their time wrestling with data rather than unlocking business value. So our industry is starting to realize we've been trying to modernize for a long time, but we're not seeing the return on investment yet. It's time to think differently. It's time to start weaving the way we think about software more fundamentally into how we operate as a business, to empower smaller teams, to reduce complexity and have significant business impact, all while reducing the sprawling cost and consolidating with a higher return on investment.
Our developer data platform vision is all about bringing that idea to life. It's all about the idea that you should have a singular place in which you can build for the vast majority of your software needs. You've probably seen this diagram or a diagram like it, the idea here is whatever software application you're building up top, it should be really easy to have that application connect through this wonderful elegant abstraction, the document model, which we'll talk about later and the MongoDB query API. And behind the scenes to be able to have the database, drive the wide variety of workload shapes that bring that application to life, from the bread and butter, operational, transactional, the full-tech search and time series and other capabilities, which we'll talk about.
And we, of course, do this on a secure, resilient multi-cloud foundation. That's kind of the core idea for us, and this allows small teams to build fast and to be able to unlock enormous business value as a result of how quickly they can do so even though they have the power to scale.
But I want to double click into the first box, kind of the 10,000-pound gorilla, operational transactional. This is the workhorse that sits behind the software-defined economy. Every transaction, every engagement, all of you typing on a live application. Every single interaction throughout your life and behind the scenes on the business -- any internal business system. All of that lives and breathes and dies inside the operational transactional database layer.
This is a huge part of the digital economy, and it's somewhat hidden away by all the applications we use. And if we think about the time in which MongoDB was born 15 years ago, at that time, it was so clear that the relational database model that had been pioneered half a century earlier was not fit for purpose for the needs of the moment. It was simply too inflexible, meaning you had to figure out what your software was going to do upfront and you couldn't change it from there. We're all conditioned to expect software to continuously evolve to our needs and our business needs. And this model inhibited that from happening.
The data model was also simply less able to handle the flexibility and boundless fidelity of the real world. When you think about how software today has reached every nook and cranny of our lives, that's made possible because software can express those ideas and relational database makes it very difficult to do that. It wasn't able to handle Internet scale, couldn't handle globally distributed use cases. And the cost model was extremely punitive. The hardware scaled vertically, which led to exponential cost dynamics rather than the linear cost dynamics that our industry needed to see instead.
So our whole philosophy with operational transactional has always been an amazing developer experience because as the developer, the artisan of the back end of this software can't easily build the software quickly, what's it all for? They're never going to use it. And that's centered on the document model. We have to make it secure and easy for these teams to build fast and the distributed system makes it very reasonable to have a resilient and scalable foundation. This was a core differentiator for MongoDB since the beginning. And of course, this core differentiator as well in being able to run it anywhere. In the on-prem data center or in the public cloud in Atlas.
And if we think about the fundamental reasons why the document model is so powerful. It's actually across a couple of key dimensions that are highly complementary. The first is the developer experience. The fact that a developer can think in their code naturally, write an object in their code, [ Jsonic ], which is the lingual font of the Internet, be able to write that to the database in a first-class way, it allows a developer to feel like they can fly. It's totally natural.
But the flexibility that I was getting at before allows you to model the world as it evolves. And this is so important. If you think about what I'm saying to you right now, whatever you're internalizing from this conversation, it's not going into tables in your mind, it's going into shapes of data that are more natural for you to think about and categorize and the document model lets you do exactly that naturally.
But here's what's really interesting. The document model actually has profound advantages for performance and scalability as well. This is where it's kind of like having your cake and eat it too. The document model stores the data the way it's accessed. And this is what allows for that linear cost scaling that's so important for infinite scale use cases today. This is what a document looks like. You can see top level fields, self-descriptive, different kinds of data inside. You can see the power of embedded arrays or lists of sub documents. This structure, which is -- for illustrative example is arbitrary, but it could be any structure, is extremely liberating for people.
The power of MongoDB comes largely from the ability to layer in indexes on top of this. These indexes allow you to drive those different shapes of workloads that drive those different kinds of software applications on top, the transactional, the search. And so in this case, we might have textual information, we might have metadata about images or songs or videos. And we could even have our vector in beddings field, the one that's highlighted and we can layer in on top of that of vector search index, which we'll talk about later, all integrated into the power of the document model.
So the power of this platform has allowed us to feel incredible privilege to have incredible companies around the world that have built amazing applications and in many ways, core transactional systems on top of this platform. What's so amazing about this is these are companies everywhere in the world, in every industry at every stage of company. From someone learning to code for the first time all the way up to the core transactional banking application that you may be using literally right now.
So for us, we have to keep investing in that core and that transactional capability. And MongoDB 8.0, which Sahir announced today, brings major improvements to performance across the board, including for time series workloads, major improvements to the rebalancing speed for sharding as well as a continuation of our first-to-industry Queryable Encryption road map, with the addition of range support coming soon.
So now let's shift gears to full-tech search, such an important workload shape native in our platform. Let's talk about the alternative. The alternative is a world in which your database is completely separate from your search engine. This is what people did for years. They'd have to learn how to manage 2 systems, move data between them, they have to manage different APIs, pay for this complexity, pay for the governance overhead. It was inflexible.
MongoDB Atlas with Atlas Search unifies those into integrated experience for the application, one endpoint, one system to manage all the power of search, combined with the transactional system, reduces that synchronization tags, move faster, superior developer experience, reduced governance overhead, enterprise-ready in a fully managed platform. Our customers love this. They've been able to, in many cases, consolidate, replace search engines and put this into Atlas Search in an integrated system and also our customers are simply able to build faster.
They can reach market faster. They can have developers that don't have to learn a whole different system. They're ready to go immediately to build great search use cases. And by the way, search drives enormous business value, next best action and countless context.
So on the search road map front, we've been doing a ton. The most important thing to be aware of is that we've layered in something called search nodes over the last year. Those are now generally available on AWS and Google Cloud, and we were excited to announce today that they're now in public preview on Azure. So they're available on all 3 cloud providers today. Search nodes deliver independent scaling on optimized hardware on the back end of those database clusters crucial for at scale applications.
We also made a really exciting announcement today, a big one, which is that search will be coming to MongoDB community later this year. MongoDB community is, in many ways, for us, the top of the funnel, millions of developers around the world everywhere, are downloading the database getting started, writing code on their laptops and bringing this extremely powerful capabilities, a strong differentiator of search to those people everywhere they are, we think is a key differentiator for our platform.
Shifting to vector search. You probably know that Vector search is related to generative AI applications. The canonical application architecture of the moment -- retrieval augmented generation or RAG. The idea of building these applications has a couple of key steps to keep in mind. You're going to have your source data, typically in your operational data store, and you're going to create vector embeddings to summarize that data. You're going to take that natural language prompt from, say, an end user and you're going to use that to search via vector search to find the relevant context that was generated into a vector embedding and you're going to use that to pull back the relevant context from the operational data store pre-vector embedding creation. And you're going to push that through a prompt engineering pipeline through your large language model, which in turn will give you a cogent response to send back to the end user. This is kind of the typical RAG architecture, greatly simplified.
Well, for us, it was just such a natural and obvious consolidation to say that vector search and the operational data store should be one. Cohesive platform and that's, of course, what we announced last year.
The retrieval augmented generation workflow is fundamentally about the power of combining -- people always say it's really just about vector databases, actually, it's about combining the power of vector with the document model and other classes of queries to do easy role-based access control and filters so that you can easily reason about the security model.
So for example, if someone in this room is only authorized to certain classes of data, we can so easily understand how to only push that data into the prompt engineering pipeline. And we can combine that power with a vector search query to find exactly what's relevant to them. Our customers love the fact that this is an enterprise-ready platform where Atlas is already approved, they can build RAG applications, Gen AI Hackathons, et cetera, literally right now and they can move fast.
The document model is a better fit for RAG than -- or put another way, I would say RAG drives the value of the document model more than I've ever seen because the power of that ability to model the fidelity of the real world is particularly compelling in a world in which you need to chunk into the document all the context relevant to answer the question.
So data modeling is core to RAG and the document model is critical to unlocking that. We were excited that Retool did a survey last year, and we actually came into the leader quadrant for both the popularity and Net Promoter Score dimensions, and this is before we were even generally available. We, of course, moved into general availability late last year. Our customers love the value. They love the fact that they can build fast in an integrated platform. They've been able to put to use knowledge agent type use cases that have unlocked material business value for them quickly on top of this platform in many cases.
We've been doing a ton of investment around Vector Search, as you can imagine. So much going on here, including optimize dedicated vector search nodes on the back end of those database clusters with optimized hardware that's independently scaling, also generally available on AWS and Google Cloud today also came to public preview on Azure today. We announced a deeper integration with Amazon's Bedrock service. We now have first-class support for their knowledge base and their agents framework. And of course, as well, we announced that Vector Search will be coming to MongoDB community later this year, enabling those millions of developers to build these modern applications wherever they are.
Shifting to stream processing. I think we all know that the real world is not static. It's living, it's breathing, it's pulsating with information. Whether a digital application where every user action is being recorded to be learned upon later or a power plant or a manufacturing assembly line. There's countless use cases for streaming data to be put to use in software for software to communicate with other software and many other use cases. And if you look at the core components of a streaming system, there's the stream transport layer. This is basically the pipes that move data around. There's the stream processing layer that will take data off those pipes and transform it and put it to use in a database, which is where it can be put to use in software. And you can move both directions through the system.
We realized it was such a natural expansion for us to move towards stream processing because it's going to make it easier for our customers that are taking advantage of streaming to do so. In particular, the document data model is such a perfect fit for this type of data. When you have a stream coming off any context, the fidelity of that data, again, it's going to be flexible. And we saw other systems are very rigid for this. And it was just -- this is our sweet spot. Let's do this. So we, of course, have an integrated experience, reducing the operational overhead, making it easier for people to build quickly without feeling the tax.
During our public preview, we've seen great customer validation, folks pushing through hundreds of millions of events per month and just feeling like again, they are developers who are already MongoDB experts, didn't have to go become experts in a whole different system to build applications straight off the streams in their enterprises. So today, huge announcement, Atlas Stream Processing moved to general availability. We started with support for AWS. We'll be expanding to Azure and Google Cloud in the future, and we integrate seamlessly with Apache Kafka, Confluent, Amazon's managed Kafka service and other Kafka compatible solutions. Over time, you'll see us add more and more integration points there.
And final topic I wanted to talk to you today about today is Edge. We all know that cloud computing has transformed everything in our industry. Giant near limitless amounts of compute available hundreds of miles away and data centers that we don't have to worry about. But in parallel, to all this innovation in cloud, there's been a lot of innovation in terms of compute that runs inside the Edge. The Edge could be a car, it could be an airport, a hospital, a stadium, an event center like this one. As we have more and more powerful compute locally, higher bandwidth connections between the compute locally, there's all kinds of cool things that we can start doing.
There's many benefits here. We can -- imagine orchestrating a symphony inside a location like this conference center. We can have lots of different systems in real time, low latency, continuous availability, doing all kinds of powerful experiences that enrich our lives in a variety of ways. Sometimes critical to core aspects of our society like hospitals and airports like I mentioned before. And sometimes, all kinds of other things that might be more temporary like music festivals and everything in between. But traditionally, it's been very difficult to build these applications. You had to think about a completely different stack for your public cloud environment than for what you were going to implement in the Edge.
You had to deal with challenges of lots of different hardware and software that was going to require different runtime environments and lots of patching and security challenges to deal with as well. And while these problems are never going to go away completely, that's the nature of Edge, we have something now called the Atlas Edge Server that changes the game here quite a bit. Atlas Edge Server is a software solution. It runs on top of your hardware in the Edge environment. And what -- you can think of it as basically bringing a mini Mongo down to your Edge environment that can be used to be that conductor of that symphony locally, all the devices locally, kiosks, IoT sensors in the manufacturing assembly line.
Any number of other examples can all interface locally with that edge server, with low latency, real time, bringing that processing and increasingly, of course, inference into that Edge environment. You can have many of these satellites all over the world, all synchronizing back up to MongoDB Atlas. We deliver real-time sync bidirectional, meaning you can make a change in Atlas and have it show up in the Edge or vice versa, optimized for these kinds of use cases.
Today, we launched Atlas Edge Server into public preview. We were in private preview last fall, and we're really excited to get to the next level with customers starting to bring this to life in a variety of new ways. So that's the end of my section.
With that, I'd like to invite to the stage we're MongoDB's Field CTO, Modernization Factory, Paul Done.
Paul Done
Thanks. Hi, everyone. Good afternoon. Really pleased to be here today. So my name is Paul Done. I'm -- for the last half of the year, I've taken on the role of field CTO for a new program of work, we call Modernization Factory. I've actually been at MongoDB for about 10.5 years now. Previously, I was distinguished solutions architect working out in the field with many of our customers.
So you might ask what is Modernization Factory. It's basically a program of work internally where we're looking at how we can help our customers accelerate moving their legacy relational workloads and applications to MongoDB by leveraging AI. So as you can see on the slide, I've got a long history in database industry. I actually started at Oracle as a graduate engineer back in the mid-90s and then moved on to various notable companies where I was building enterprise Java applications working against Oracle and other relational databases. And then following Oracle's acquisition to BEA Systems, the home of WebLogic in 2008, I came back into Oracle then.
And that experience with relational databases over the years is really helping -- give me the sort of insights required to look at how we help modernize our customers' legacy applications from a rational to MongoDB for this program of work. So we've talked about this before, and it's nothing new, but our principal competitor remains legacy relational technology. And for the last 10 years, when I've been out in the field working with our customers, I've been seeing this daily where our customers have been choosing for net new applications, a database for their application. Invariably, our competition has been relational databases 4 times out of 5, I would say our competition is relational databases and not NoSQL databases. And in those competitive situations, we tend to win.
And so what's the challenges with relational technology? Well, they were invented 50 years ago. And in this modern world, relational databases are constrained by many of the assumptions people made about computers way back then. Data tends to evolve over time and the rigid schemas of relational databases resist and inhibit that change. And increasingly, modern applications and their databases need to be available 24/7 and be able to scale quickly to deal with fluctuating demand. Relational databases were built typically to run on a single machine to service a very fixed single workload back when taking the system offline to patch was perfectly acceptable. And these things are untenable for the applications of the modern world.
So the obvious question is then, if like I've been seeing out in the field, we're typically winning most of those net new workloads against relational, why is relational technology still dominant and still has a dominant market share? Well, the reality is most applications out there in the enterprise aren't new. They've been actually running for decades. And decades ago, when people were choosing their database for applications, the only viable option out there for a database was a relational database back then.
And also getting off relational databases is just hard. So let me try and explain why. There's a tremendous variety. One tends to think about relational applications as all being the same. There's actually a massive variety when you look at one relational application and then the next and how it's composed. Each one is very different. So let me try and give you a sense of why that is.
So if we think about a legacy application in enterprise, first of all, the database was chosen for that. Was it Oracle? Was it SQL Server? Was it Sybase? Each of them have subtle different variants of flavors of SQL they support and in each of them, we'll have things like stored procedures and triggers for embedded business logic that actually use completely different languages for that. So you've got that variance, just the database that was chosen for that legacy application.
And then what programming language was chosen to implement that application? Perhaps it was Java, maybe it was COBOL or C Sharp or one of many other languages. There's another variance you've got to deal with across each legacy application you're looking to modernize. And then there's a run time, when the application is deployed and running in production, it's typically running on some sort of application server. And even in the Java space, there's millions of different types of application servers that have different APIs and different idiosyncrasies and they force the application to be written differently depending on what applications server that application is running on.
And then in the code that was written in that legacy application, there is a myriad of different ways that, that could be integrating with the relational database. Perhaps someone wrote the code using direct SQL over ODBC to talk to the database, perhaps they're trying to leverage stored procedures that they're writing directly into the database or maybe they're using an object-relational mapping tool like hibernate and ORM to talk to the database. So yet another choice, another layer in the same application that can vary wildly.
And then for that legacy application back in the day, how do they choose the user interface? Maybe it was a 1990s based old data input screen-based technology like Oracle Forms for the UI. Or maybe it was in the 2000s, it was a native Windows desktop application that was built for that or maybe it is a more modern web application, but maybe it's using Java Server Pages to implement that or maybe it's using ASP from Microsoft.net to implement that. Again, a lot of variance. And so you can start to see that each application that might have been constructed back in the day might weave its way between different components in each of these layers.
And the reality is each of these layers isn't four different choices, it's typically tens or hundreds, depending on which layer you're looking at. So the complexity of when you come to try and look at the challenge about how we help customers modernize their applications, we have to deal with this complexity and think about how we tackle this complexity.
So it's worth reminding you what are typically the main steps in modernizing the legacy application from relational to MongoDB. And we've talked about this before. First of all, you've got to think about how you take that relational data model and map it to a document model for MongoDB. And then you've got the really hard part, how do you actually rewrite all the application code through those layers that I've just talked about on the previous slide that have a lot of variance.
And then once you're ready with that migrated application, you've got to deploy it and you've got to move your data over quickly running legacy relational database to your new now MongoDB deployed database with as little time as possible. So that was a high-level view, but it's worth going in a level deeper to think about what are the main steps involved to do that migration? And it's worth doing that because as I think about modernization factory and how we can leverage AI to accelerate this process, we need to look at the different parts involved to see where we can apply AI in each stage to provide that acceleration.
So first of all, when we're looking at a legacy relational application, we need to analyze that code base. We need to go and look through all that code and try and understand what was the technology stack there like we showed in the other slide? What's the component structure look like? What is its architecture. Until we know that, we can't move on in the next phases of the migration.
And then we need to do create end-to-end tests. And probably a surprise to no one in this room, most legacy applications don't have test. It wasn't a thing people did back in the day. The people didn't see the value. It was too much hassle. And so a lot of these legacy applications don't have any tests. But if you don't have tests for the legacy application and then you come to migrate it, how are our customers going to feel assured that what's been migrated is fit for purpose. So that slows things down and it introduces risk.
Then we need to actually analyze the architecture and think about the target state, and this is where you might get into going from a relational model to a document model. But it's also when you start to think about, hey, this is a monolith. And as we bring this to MongoDB and the migration, we probably want to modernize to something like microservices as well to get the full benefits of modernization. So you've got to deal for all those steps in that design phase. Then you get to actually having to do the rewrite and recoding and adding new coding.
So you're partly changing code in all these layers to instead of dealing with a relational model and talking to the APIs of the relational database or via the ORM, you're actually changing that to use different APIs to talk to MongoDB using a completely different model, a document model. But then as I talked about before, there's probably other pressures of why you're modernizing and there's probably other aspects to that modernization that is not just about getting to MongoDB where you actually need to leverage other newer capabilities for modernization. So that brings in the need to bring in more code and different code during that rewrite.
And then you're back to the user acceptance testing, which still happens and those users are going to do that testing. And that's going to win the books in what you've migrated. It's still not fit for purpose yet. And then you're going to have to address all those books. And finally, once you have that application ready and migrated from a code-based perspective, you actually have to do the switch over. How do you do that in such a way where you're going to go from a running production system running against a relational database to this new migrated production system on MongoDB as quick as possible with as little downtime. So as you can see here, there's a lot of work involved and a lot of variability.
So last year, we announced the general availability of Relational Migrator product, and it's a product that really helps simplify modernization. So we think -- if we look at those steps we just showed before, where does Relational Migrator product today help? It helps a lot with design solution architecture. So being able to take that visually that relational model and start to say dragging and dropping how that maps to the document model and capturing that. That's now a lot easier, thanks to Relational Migrator. And towards the end of the project, when you've actually deployed that application and you've got to move that data as quickly as possible from that legacy Oracle or Sybase database to MongoDB, it enables you to do that very quickly, a push of a button and live migrate over, minimizing any downtime and providing that safety, the validation that you're not losing data or introducing corruption.
And as you've heard probably earlier today, we've also started to do a first foray in Relational Migrator into helping us to rewrite the app code, specifically around helping restore procedures and triggers, things like PL/SQL or PL/SQL in Oracle and how we can convert that quickly. But as you saw before, there's a whole load of other languages and composition and components in that application architecture that's still not necessarily being dealt with today that, overtime, we will look at further and further.
So the response since we GA Relational Migrator last year has been really tremendous. So developers really love the fact that we provide an official monolith to help you visually map from the relational model to the document model. It's really easy thing to use. It's really intuitive. And then the other thing is, in the past to be able to move data from one database to another, it takes a lot of engineering to validate and make sure that's right and reduce -- eliminate any risks. So we provide all that plumbing now, and they really like that.
The ability to click the button, synchronize the data and have all that validation and checks in place to ensure that nothing is happening to that data, and it's moving over as quickly as possible. However, we can make app modernization much easier, thanks to AI.
So over the last few months, we started running several pilots over the last half year and going into this year to try and make this much easier. And I think you heard in the keynote a bit earlier, one of those pilots was Bendigo Bank, a big retail bank in Australia. There's another pilot been going on for one of our customers, a Swiss private bank. And we've got many other pilots just starting to spin up at the moment and looking at how we can leverage AI to help with all those steps in terms of accelerating them and making modernization and migrations much quicker.
So how are we doing that? If we think about -- if we look again at some of those activities involved typically in modernization today when you're doing it manually, how are we believing and how we're starting to see that we can accelerate them with AI. So we talked before about analyzing a legacy system where a developer might have to spend days or weeks going through line by line all the code base that they've never seen before, and they don't understand. LLM do that easy. That's really straightforward from LLM to look at a code base and actually tell you how it's constituted, the technology stack, the componentry.
And this is doubly important for legacy systems because the reality is, in most of our customers the developers that develop that legacy system aren't in that team anymore. In fact, it's normally worse than that. They've actually retired or even worse. They're not even in the company. And so that knowledge has gone. And so the LLMs don't need that. They can actually go back to the source and make sense of that. So that really speeds things up that might be taking weeks or months to do and bring it down to a matter of minutes.
And then creating the end-to-end test. What we're finding is we can record a user using that existing legacy system, maybe it's a web application and we can record their interactions. We take a log of that. We can then feed that into the LLM, the large language model, the AI model and that can make sense of that and then generate the functional tests that recreate those interactions. So all of a sudden, very quickly, we've got complete test coverage over a legacy application that never had that before. And that then, when we come to do the migration, we rerun those same generated tests against the migrated app and be assured that, that actually now is fit for purpose and working. So that derisked a lot of things that actually collapses a lot of the later cycles when it comes to testing as well.
When it comes to design solution architecture, I mentioned it before, one of the things typically our customers want to do as well as moving to MongoDB, they want to move from a monolithic architecture to a microservices architecture to give more agility and to deal with business change more quickly. And using LLM, we can analyze that code base and the behaviors of users using it and make much more sense of the domain boundaries and what these microservices should look like in a future design, which is actually quite a hard task for a human to do. And then, of course, coming to rewriting the code the bit you would expect. It's much easier to take all that source code base and the behavior and feed that into the LLM and actually generate these new microservices, the new components and a large part of the implementation of those.
And then when it comes to user testing, of course, you still need to use as human users doing user acceptance testing. But then the books that they find and they log we can actually feed that back into the LLM and it can actually derive what the solution is to fix for that book and then give us the code to apply that. So that's where we're getting acceleration as well. And then lastly, when it comes to actually migrating and deploying, in reality, most customers that are modernizing applications do this in phased approach so they're going to take component by component out and go live with that new component.
So what you tend to find is, over a period of time, you have a hybrid modernized and legacy application coexisting together that is partly using the old relational database and partly using the new MongoDB database. So essentially dual running. And LLM can help us with informing us how to structure those interim systems as we go on that path to finally dropping the relational database and having the fully migrated application. So on these pilots and the other ones going on now, we've got some early lessons that I thought it might be worth me sharing with everyone here, some interesting ones.
So I've talked about this quite a lot, but it's so important, and we're getting such a benefit from it. I want to emphasize it again, developers hate writing tests. And when they write tests, they don't tend to do it very well, and they tend to be lazy, and they don't do that coverage. So as well as finding AI is massively accelerating, recording what users are doing and automatically generating the tests. We're actually getting better test output than that actually humans are doing, so not just acceleration, but better quality that's going to help and derisk.
Also, as we've been working -- as we're working on these different pilots, we're dealing with different LLMs and different cloud providers LLMs and other providers LLMs. And not all LLMs we're finding are built equally. Some LLMs we're finding aren't very good for modernization tasks. Some are good for some types of tasks we've talked about and some are good for others. So we're starting to amass that knowledge of what LLMs to use in what situations, and you can imagine that can start to inform some of our future playbooks in terms of best practices.
And then the last thing that I find important as well is there's a slight concern out there that maybe what AI generates and generative AI generates isn't always 100% correct. And that is true, and it's absolutely fine. So the way we're doing this is if we're using AI in any of these steps to generate content and accelerate that, we're still using an expert for the last-mile validation and the last-mile book fixing there, those last-minute changes. So it ensures when we're doing this process, the quality of the output, the quality of the migration is exactly the same as if humans and experts have done this completely manually. However, at the end of the day, we've needed less experts to do this for a lot less time. And that's where -- that's the benefit of the acceleration of AI.
So as we double down on accelerating modernization factory, what are we going to be doing from here? So throughout the rest of this year, we're going to double down on focusing on these pilots that we're running. Deep engagements with our customers where we're experimenting together about how for these different variances of applications and their composition, we can apply AI in different ways to accelerate that. And we're learning a lot, and the customer is going to learn a lot.
And as we go into the future, we'll expect that to manifest then in two ways those lessons learned. One, we'll expect to be building out playbooks of those best practices and templates and starter toolkits that we can then take into future projects with future customers where lessons have already been learned and accelerate their migrations, a flywheel effect as we go along. But of course, we have the Relational Migrator product. And everything we're learning here is going to be really important for feeding back into that product and driving roadmap where we can actually then have accelerations like this out of the box that every customer would benefit from automatically.
So in summary, getting off relational is really hard, but we're learning how to -- we are now learning how to harness the power of AI to make the process much easier. So hopefully, that was useful. I'm now going to hand over to our Chief Product Officer, Sahir Azam, who's going to talk to you about how we establish ourselves as a trusted AI partner. Thank you.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Good afternoon, everyone. Thank you for joining us. Hopefully, that gave a good glimpse of some of the exciting work we're doing on kind of the front end of kind of research and application of some of the technologies that we all hear about every day right now in the news but in a really pragmatic and practical way.
So the last piece I want to talk about is how AI also presents a way for us to drive that standardization in a faster path to that than we ever had before. Michael walked you through those different journeys. And one of the things we're seeing is that, first and foremost, Generative AI, in particular, has made this a Board-level conversation. We've had executives that we've never had access to before sort of engage with us. It's very much top-down driven from organizations, which is actually quite interesting because oftentimes, this platform shift gets compared to when the cloud first came about and really started to launch. And when the cloud started, it was very bottoms up. It was shadow IT. It was development teams going around central IT operations, getting started. And eventually, that value was evident and then it became more of a C-level or executive-level drive.
AI is a bit different. It's actually starting from the top in terms of the opportunity to drive efficiency and amazing new customer experiences. But we are also hearing from our customers that they're struggling to formulate and understand what the correct strategy is or even what the right use cases are to drive the best ROI. And as you all know, the ecosystem is moving quite quickly. I talked about sort of a simplified version of our ecosystem of different technologies and tools that are out there in the generative AI stack. You see a lot of analysts, a lot of venture capital firms publishing kind of this idea of an emerging AI stack. Everything is moving very, very quickly.
Even within the venture community, where most of the new funding is going, it's all around these AI-based technologies and how you empower organizations to use them. This, of course, is complicated for the average enterprise to figure out, okay, how do I get started? And where do I pick bets that don't lock me into a particular approach, but give me flexibility. This extends not just in the stacks, but actually at the models themselves. And just in the last few weeks, Mistral launched an amazing new open source model. Then the next day Meta releases Llama 3, and everyone's buzzing about the fact that Llama 3 is now almost as good as not equal to Open AI's latest models. There's a lot of excitement around what GPT-5 might mean. So this race is happening very, very quickly, and we know there's billions of dollars of capital going in.
But navigating this, of course, what the right use case is to apply which model is complicated. There isn't sort of a standardized way to do this yet. And we're actually starting to see data come out around this as well. And we found that when executives were recently surveyed, 87% felt that they weren't necessarily equipped to manage transition to AI.
So on one hand, they know it can be crucially impactful in terms of driving efficiency, compelling new customer experiences, et cetera, but yet getting started and navigating this complexity is quite a challenge. But as we can observe what enterprises need and what the bleeding edge companies that are starting to have success are saying, there's a few key themes that kind of emerge. One, I think there's general understanding that AI will, over time, apply to a variety of different use cases across large organizations. This could be everything from efficiency in a call center or customer support functions, which I think we're all starting to see some great case studies in the industry to actually helping with creative life cycles. I had Adobe on the stage earlier today, some of [ the AIR ] tools and technologies around generating imagery and content. So there's really a lot of excitement there.
Two, what we talked about with Novo Nordisk. So being able to shrink the time to submit drugs for approvals, it's all sorts of use cases here that are evolving, which then in turn means that there are different needs for different models. Various models are stronger at different use cases. Oftentimes, they need to be fine-tuned with the customers' data to be even sharper for a particular organization's application. And there's trade-offs right now, especially between latency and cost. There are smaller models that might execute really fast.
There are large models that are super capable, but are higher latency and higher costs, navigating all of that and mapping that to these use cases is something that customers are starting to build understanding around. We are also seeing billions of dollars being invested by the hyperscalers, our 3 key partners, not only at the underlying infrastructure compute layer, but in tooling, right, to make it easier for customers to get started. And for the first time, we're actually starting to see some divergence even in the differentiation between these cloud providers. And so for a customer who's got knowledge, information, IP and their data. The ability to leverage the cloud services now from multiple providers is even more important than the early stages of multi-cloud that we've been talking about for the last couple of years.
And for the first time, enterprise data can truly be unlocked to power applications. And what I mean by this is the majority of today's applications are leveraging structured or semi-structured information that can fit in the database and be reasoned about and queried. But because of new approaches with AI, now you can build applications on top of unstructured information. Text, obviously, with all these large language models we've seen. But even now multimodal kind of models come out that can work with audio and video, and that's only going to increase over time. And yes, we all know that applications have had images and things like that for some time. But now it's really about the understanding contained within all this information that can power an application like never before.
So these kind of 4 factors really in our eyes set up MongoDB quite well. And some of this is luck, some of this is intention. So one, we have quite a bit of expertise. We moved really fast in applying generative AI, frankly, first for our own internal usage. We have technologies that our customers support agents, customer success agents use to get quick answers for customers. So the turnaround time and customer engagements is faster. Our support organization can now create summaries of an incident, a postmortem when something goes wrong, much faster than they ever were able to before.
Our go-to-market teams have an internal Slack bot that we call ChatGPT -- CoachGTM, sorry, all these acronyms. And basically, it allows them to find answers quickly. So if they're customers saying, "Hey, I want to deploy Atlas? Is it available in this particular region?" Boom. They don't have to go through documentation or enable the materials, they can just find that answer super quickly. These are just a handful of some of our use cases. And now we can leverage that knowledge to help empower customers to do -- drive similar use cases in their organization.
The other area that we bring the expertise is, frankly, with our partners. We've been spending a lot of time myself personally, Andrew, a bunch of folks in our organization, with the venture-backed companies that are leading the way in this ecosystem, you'll hear from some of them today to really get an understanding of the latest of what's coming and what can be applied today versus what's more futuristic. We've also built broad partnerships. We're model agnostic. You heard from Anthropic today on stage with Dave, but we also work with the models from the major cloud providers. Cohere is here. So we want to make sure our technology is completely open to support that wide variety of use cases and the models that back them.
We are, as you know, ahead of the market in cloud independence. We're in 117 regions today across AWS, Azure and GCP. We're the only technology that can move data in our operational database and mix and match across all those 117 regions across cloud providers. So that independence to say, "I'm an organization to power AI, I need my data to meet the customers where they are, where the models where the rest of the stack is and be able to fluidly move that as a key advantage."
And as you heard from Andrew and a bit from Dave earlier today as well on stage, our data model, the foundation and core of the product is architecturally built in a way that makes it very easy to manage heterogeneous data. And so this sets us up really well in terms of becoming a thought leader and a partner to our customers at a very high level. And they're looking for us to be that trusted adviser.
And that has a direct connection to becoming a standard. As we've been talking about, as Michael mentioned earlier, the motion for an operational database tends to be now in the modern era, a very bottoms-up developer-driven sort of motion. So we tend to land with a specific application that has a sharp acute pain or developers that really want to work in a modern way. We build up some momentum across multiple apps within that team, then start to spread across teams. And eventually, we get to a point like those examples where we have a standardization and we've seen an inflection point where we go from a few dozen applications perhaps to hundreds or thousands, as you heard from some of the customers earlier today.
But this ability to position high in an account and get to those executive level relationships faster is really compelling for us. And AI is a way to do that to engage in a more top-down way than ever before as an operational database technology, fundamentally. And as evidence of this, just 2 weeks ago, maybe 3, I was in London, along with Dave and a few others from our team. And we pulled together an event that was really targeted at C-level executives across large enterprise. So finance, retail, we had some tech firms there as well and government agencies.
I think we have folks from 10 or 11 countries across Europe come together. And the goal was not to sell MongoDB. The goal was to bring together executives to talk about where they are in thinking about AI, the safety, the risks, the regulations around it, the ethics, what are the use cases that are driving value and get everyone in the room kind of talking about this more as business leaders fundamentally as opposed to just starting with the technology conversation.
And I was at one of these tables and somewhere in that picture maybe. And actually, it was interesting. I was sitting next to a business leader from one of the major global banks and she's actually has nothing to do with the MongoDB adoption in the organization. The technology teams, obviously, are large-scale MongoDB adopters. We've been in that account for a decade, but she actually was completely thinking about it from a business perspective. How could she transform her organization, what use case is. So we're getting access, not only higher, but more broad with executives than I think, because of this ability to be this trusted partner. Now obviously, we are not just letting this come to us.
We're being very intentional in a variety of different ways to make sure we can capture this opportunity. So as a technology company, no question. It starts with the product. So as you know, we've made a bunch of announcements this week. But for the last 18 months, we've been investing aggressively across every facet of our portfolio. So whether that's fundamental optimizations in the core database, our Vector store, et cetera, to make it more performant, more capable and more cost effective around these use cases, a broad range of integrations with a variety of different layers of those emerging AI stack, as I talked about earlier. And we focused on really identifying use cases that are natural fit for our technology and where we can help customers.
And obviously, generating code, managing code, you heard about modernization is a great use for where these technologies are today and their evolution. So this will obviously continue. This is just a glimpse of some of the stuff we've been up to over the last year or so. We're also continuing to expand our partnership. So I would say the last 18 months was largely about breadth. How do we make sure that we give customers the flexibility no matter which emerging developer frameworks or stacks they're using, which models they're using. And we feel like we're in a good spot there, but we will always continue as things get evolved to maintain that breadth. However, for enterprises, they want to solution that's much more prescriptive. They want our knowledge, our backing of the right stack to use for various use cases. And so we announced this new program, this MongoDB AI Applications Program, MAAP, as we call it for short, internally.
And this is a combination of first technology, so Atlas, the core database, integrated, validated into the surrounding ecosystem with the key leading partners in each of those different segments, reference architectures that we can all stand behind and are proven in terms of their deployability and of course, a set of services and knowledge with experts. And we really are looking at this in two ways: One, strategy development. This is more around organizations that may not even know which use case to go after first. So we bring in more of a broad-based for you to say, hey, here's what we're seeing at other customers. Here's where we've had success. Here's what our partners have seen value for other organization. Here's perhaps some use cases to show quick ROI and get started or we can get very hands on. And there are times when customers know, this is a particular problem I want to solve. I want to make it easier for my support staff to answer questions quickly for support calls.
And in that case, we can actually get really hands on and start prototyping applications and get started more on the technical level. So these are some of the ways we're getting engaged. This is a combination of MongoDB services, but also our boutique SI partners who have specialization in AI, and they've already started delivering some projects.
So net-net, there's a variety of different lenses and things that are happening in the ecosystem and within MongoDB that we see that are allowing us in the market to become much more of an implicit standard. And we're excited about the potential that will have in terms of the position, especially in some of the largest organizations, the most complex organizations in the world that have thousands and thousands of applications.
So I'm going to switch gears a little bit.
So obviously, today, both in the keynotes and in the session, you're hearing a lot about ecosystem, a lot about technology partnerships. I mentioned we're out there learning as much as we can from the most interesting kind of early-stage companies, later-stage companies that are doing amazing things. And to get you a glimpse of that. I want to invite 3 of our strategic partners. These are founders and executives who started these companies to have a little bit of a discussion around what we see in the state of AI-driven applications. Come out folks. Maybe just the middle 3. Yes.
All right. Thank you for joining us in person. I know it's tough to get away from running and operating an early-stage company and all the demands that it had takes as an executive team. So really, really appreciate it. It'd be great for you all to first kind of give it a little bit about yourselves, your company, the mission of what you're focused on, Brandon. Let's start with you at the end, and then we'll work this way.
Brandon Duderstadt
Yes. Hi, everyone. My name is Brandon Duderstadt. I'm the Co-Founder and CEO of Nomic AI. At Nomic, we're focused on building tools that help make AI accessible and explainable. Concretely, this manifests as 3 product offerings right now. We have Nomic Atlas, which is a tool that makes it easy to interact with and collaborate on massive unstructured data sets in your web browser. We also have Nomic Embed, which is a model that allows you to represent unstructured data as a vector. It's how you get the vectors for Vector Search. And finally, we have GPT for all, which is an open source ecosystem of low resource language models that enables you to run things like Llama 3 on your local laptop and other bespoke hardware.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Lin, how about you?
Lin QiaoFireworks.ai, Inc. · CEO, CFO & Secretary
Hey, everyone. Very nice to meet you all. I'm Lin. I'm CEO, Co-Founder of Fireworks.ai. We're strategic partner to MongoDB, very -- a lot of owner for us to work together. I have been -- before starting Fireworks, I've been working at Meta for many, many years together with my founding team. We have -- I was head of PyTorch that is dominating AI framework, used by industry when we talk about GenAI, all the models are PyTorch models these days. We have -- it took us 5 years to fully productionize AI infrastructure, building around PyTorch, supporting all Meta's product service area using AI, all the way from ranking recommendation to search, to translation, speech synthesis, site integrity and so on and so forth.
Our mission of starting Fireworks.ai is to bring the proprietary knowledge used by hyperscaler to make it accessible for a wider variety of developers. And second, we want to heavily compress 5 years going to production to 5 weeks to 5 days or even 1 day. So our offering is GenAI influence and fine-tuning service to all the developers in the world and enterprises in the world.
Again, those application features of real-time low latency doesn't change, whether they use GenAI or not. So we provide extremely low latency offering. And the cost of our operation running on top of GenAI is extremely high, and we significantly reduce the cost by order of magnitude. And last but not least, everyone building on top of open AI, there's no differentiation using the fundamental technology. And we help our customers heavily customizing their model using their proprietary data. So with that data, their model, they have their moat. So across [ big 3 ], quality, low latency, low cost, we offer all 3. So you can think -- in a very simplified way, you can think about us as a better version of open AI for enterprises.
Sahir AzamMongoDB, Inc. · Chief Product Officer
That's amazing. Jerry?
Junkai LiuLlamaIndex Inc. · President
Great. Yes. So at a very high level -- first, I'm Co-Founder and CEO of a company called LlamaIndex. And at a very high level, our core mission is to enable your developer teams to build LM applications over their own private sources of data. So this includes unstructured data, semi-structured data and structured data. And basically, you pick and choose all the components within the LM ecosystem, so language models like Fireworks or embedding models through Nomic and then also storage through MongoDB. And we allow you to build use cases like retrieval augmented generation, which I'm sure some of you have heard about or basically chatbot over your data, [ a gentech ] workflows and more.
And so we're the leading framework for helping you build not just stuff that kind of like works in a prototype, but actually production-grade retrieval augmented generation, that's free of hallucinations, can actually operate over your complex documents and more. So we have both an open source framework that connects all these different components as well as the enterprise service that helps specifically to clean and process your data to make -- like to create like a production-grade data pipeline that complements the open source framework.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Very cool. So as I mentioned a few minutes ago, one of the things that everyone's sort of trying to get a sense of is, what are the most interesting use cases? What are the best applications for this new technology, especially in an enterprise context? Jerry Liu, we'll stick with you for a moment and then get a perspective from the others as well on what are the most compelling use cases? What are you seeing? You're on the front end of the ecosystem, working with some of the most cutting-edge applications.
Junkai LiuLlamaIndex Inc. · President
Yes. I mean I think for us, a lot of the overall theme is around unlocking like knowledge synthesis and extraction. So basically, given your unstructured data, oftentimes, it's like just you have 1,000 PDFs or you have a million PDFs or billions of PDFs basically. And somehow, you haven't really tapped into that yet or like a human really has to go in and read everything to really give back like an answer or synthesize like a report, LLMs have the capability to basically take in all this data as long as it's processed the right way and actually give you insights from it. And really, the key trick is for your developer teams to figure out how do you go from 1 billion PDFs into insights that you can actually process.
It turns out is actually a little bit tricky. A lot of the simple things that developers find on YouTube tutorials don't really do this very well. It might work over like 10 PDFs but not over like 1 billion. And of course, besides PDFs, you have a bunch of other data sources, too. You have like Excel files, you use like Salesforce, like maybe you use Slack as well. And so you have a lot of different data and different silos. And the trick is to figure out how do you actually connect these different data sources to build even a very simple use case like a chatbot over your data. And that's probably like the thing people start with -- and people haven't done that super well quite yet. And so there's a lot of room there. And afterwards, there's more advanced things, too, like coding assistance, things that can automate like not just like synthesis and extraction, but can actually perform actions for you. And so that's where you get into the [ a gentech ] territory.
Lin QiaoFireworks.ai, Inc. · CEO, CFO & Secretary
Cool. Yes. So Fireworks, we support a lot of developers and enterprises. I would say the motion is going very interesting goes both bottoms up and top down, as Sahir just mentioned. The whole entire industry is extremely active in developed applications. I would kind of bucketize that in three categories in extremely simple way.
One is new chat experience. And that experience is either going through text or voice activated. It can be all kind of assistant. People are building legal assistant, helping lawyers to do a case study based on a vast majority of kind of public [ permission ] or people are building educational assistant to help students or me -- people like me to learn foreign languages or learn other classes, people are building medical assistant to address the shortage -- significant shortage of nurses and doctors in the medical system. They're also building customer service automation to alleviate expense of human behind the call to answer questions. There's so and so. There's a huge variety of assistant that is being built.
And second, in the e-commerce world, the interest portal of search based on keywords mostly now is changing towards chat-based. People are using natural language to interact with search experience and get the best product out of e-commerce, whether they are a marketplace or I want to order a meal or I want to buy something. So that is transitioning a large portion of the industry.
The second bucket is not generating content for chat, but more generating other kind of content, for example, Jerry briefly mentioned coding assistant. We have been supporting all kind of coding assistant, giving a completely different coding experience, whether it's supporting new program language, new coding guidance or it just make coding or code review much more efficient.
There are other things, for example, business workflow can be -- you can imagine that as a coding process and it can be automated and generated. Business metrics can be generated through visualization as well. You can extract production metrics from production logs, and so on and so forth. That's a huge bucket. And it can also generate tool cost where large language model has limited knowledge and you can call into other APIs to extract knowledge and get better answers. And the last bucket is not limited to tax generation, it's multi-modality.
The answers can be a blend of tax image or video and other medias. So we had supporting applications, building PowerPoint generation where text and images are mixed. You don't need to do the tedious work to bring up the template. It will just boom -- tell what kind of PowerPoint you want to generate for you and you can tinker on top of that. So proficient.
There are also a lot of brand marketing material, that's blend of text and images, high-quality images. There are like product catalog, cleansing, where you extract from the images of product catalog with high-quality product catalog information. So I would say there's a whole slew of disruptive, innovative application that's being developed on top of GenAI. It's a matter of time that the whole industry is going to transition massively to build on top of this.
Brandon Duderstadt
Yes. I think the phrase generative AI, I think, captures roughly half of what is going on here. It's wonderful that we can sit up here and talk about applications like the generation of content, be it code or natural language or images or what have you, but the fundamental innovation that I find myself focusing on increasingly is the fact that there's a new data primitive in town, and that's the Vector.
Up until very recently, computers have not had the ability to perform rich operations over unstructured content like text, images, videos, audio, et cetera. And that's the majority of content on the Internet. That's the majority of the kind of content that's stored already in MongoDB. And the Vector gives computers, for the first time, a data primitive that lets them perform rich operations over this. And what this is going to enable is an entirely new class of applications that can manipulate unstructured content in ways that we don't even fully understand yet.
And at Nomic, we're starting to see this firsthand with our tool, Atlas. As I mentioned earlier, Nomic Atlas is a tool that lets you interact with and collaborate on massive unstructured data sets in your web browser. That is a tool that is only possible by manipulating Vector representations of data, and we are seeing that tool have market penetration across a variety of verticals, be it pharma, consulting, defense, finance, all of these things are getting these massive unlocks from this new class of applications that are only possible because of the Vector.
Sahir AzamMongoDB, Inc. · Chief Product Officer
So how does all this change the role of a developer? If anyone wants to start. But I mean, this is like -- we talked about the stack changing earlier. You're obviously part of that stack in various ways all of you. But how do you see things being different than more traditional kind of application development process?
Junkai LiuLlamaIndex Inc. · President
I think one thing I'll actually say and this is actually to your points here during the opening remarks, which is kind of like companies like have this uncertainty basically because they realize a lot of pace of GenAI is moving very quickly, and you're trying to figure out how to like derisk and try to understand and build these use cases.
I think fundamentally, in order to do that and stay on top of the rapidly evolving landscape of GenAI, [indiscernible] and more, you have to empower your developer teams. You have to empower them with the right tools. And basically, when it comes to buy versus build, I'm obviously biased, but I have a strong preference towards build and especially building with the right infrastructure components. So the abstract away the system complexities for your developers, and they can spend their times rapidly, both protyping and productionizing any of the emerging use cases are happening. And so this includes like RAG. This includes any sort of [ a gentech ] knowledge assistance, code assistance, those types of things. But really, you want to stay on top of whenever the new open source model comes out, the new embedding model comes out, like GPT-5 comes out, you want to be able to adapt to all these things that are evolving.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Yes, it's that flexibility to be able to transition. Lin, what are you seeing?
Lin QiaoFireworks.ai, Inc. · CEO, CFO & Secretary
I see kind of one big difference before and after, like before GenAI [ active moment ] and now more than [ active moment ] on top of GenAI is -- the GenAI models are especially large language models, they are probabilistic. This is a huge difference because before application development is -- logic is encoded in code. It's more deterministic. You have full control of how the application should behave. And now you're building on top of a large language model. The model architecture is actually predictive. It's based on probability layer by layer and predict what is based on the tokens you're seeing. What is the token or words it should generate with the highest probability. So this is very different because that means you don't have full control of what the content is going to generate.
And the other interesting limitation is large language model, although we understand this capability, it's very knowledgeable, it actually has limited knowledge. Its knowledge is limited by this training data. So it doesn't have knowledge beyond that. So if you ask your question, based on probability, we'll have to give you an answer, and it's prudent to hallucinate. Jerry touched on that also.
So hallucination is not a good trait. It sometimes is extremely problematic. So the way a lot of app developers start to understand that. Oh, it's actually -- I'm losing control here, but I need to regain my control is the approach to get there is by providing relevant context and providing that as a bounding box into large language model and the technology is called RAG. And RAG basically is a way for developers to retrieve relevant information from all kind of data stored already in their accessible storage layer.
And then for [indiscernible] in feeding to large language model as the context. So the large language model can answer the question in the relevant context. And that's where Fireworks partnered closely with MongoDB because a lot of operational data already exist in MongoDB document store. And through Atlas Search, it can efficiently and accurately retrieve the relevant information and then feed into Fireworks large language model inference engine. In that way, we provide the best quality to application developers. So I think understanding the -- using those tools to best develop in new modern applications is the change that is happening now.
Brandon Duderstadt
Yes. I think the most obvious thing for me as a developer is I will never code without a copilot again. It has made me at least 10, if not 100x more efficient. And I imagine for any developer in the world, that's the same story. And so the thing you have to ask yourself is, okay, if we have just 100x more code or text or images in the world, what's the logical extension of that for applications?
And so the first sort of idea becomes as a developer, I'm faced with a new class of problems and a new class of tools. To deal with things like hallucinations, I'm going to spend a lot less time writing each character of the code and a lot more time doing analytics over my data in terms of what's going into my model, what's coming out of my models, evaluating these pipelines. And so I think that the role of the developer is going to shift away from the actual writing of the code to orchestrating more of these models that are generating massive amounts of unstructured data.
And as far as how that affects the world of MongoDB and databases, in a world where everyone is now 100x more efficient at creating data, they're going to have to have a place to store it, and they're going to have to have a place where they can efficiently operate over it. And so I think the demand for data storage and efficient data access and operation is going to increase wildly over the next 10 years.
Sahir AzamMongoDB, Inc. · Chief Product Officer
So Brandon, kind of building on that point, and then certainly, I want to -- something I want to feedback from all of you on and would be certainly relevant to this audience is, why did you choose to partner with MongoDB. Obviously, there are other data technologies. We're all very excited that you're part of this new program, the MAAP program that we're rolling out, but I'd love to hear some perspective from you in terms of you see the whole ecosystem as well. What gets you excited about we're building together?
Brandon Duderstadt
Yes. I think one of the things that's really key about MongoDB and that you mentioned earlier is the fact that it's a place where a lot of this sort of rich unstructured data already lives. It's a database that's been turned to by massive organizations to store the kind of data that's exactly what you need to operate in this new sort of generative AI world. And now with the Vector Search offering, you're going to enable people to build this fundamentally new class of applications by giving them that power to flexibly retrieve over both the Vector representations and the actual data representations that they have. And so I think the document model basically is just incredibly well suited to this new world that we're living in.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Awesome. We think so, too. Lin, obviously, we got introduced to the team on your side and started working together, I think really quickly on getting some new solutions out. I'd love to get your perspective as well.
Lin QiaoFireworks.ai, Inc. · CEO, CFO & Secretary
Yes, definitely, we're in this tidal wave of GenAI. And a lot of time, we focus on the technology side, but the real main character here is actually the new innovation application. I believe there's a slew of disruptive application that's coming out either from the startups or from incumbents transition into leverage GenAI. In order to drive this transformation, we would like to partner with the company that has the best technology and closest to developers, and that's MongoDB. And of course, MongoDB is already host a lot of operational data and provide extremely flexible schema for application developers to evolve their products.
And based on my knowledge, MongoDB has more than 0.5 billion downloads. That's amazing. And more than 47,000 paying customers. So that penetration of the ecosystem is very important for us to kind of partner and leverage that to get to helping all these developers to transition to build on top of GenAI. So that's the fundamental reason we are kind of working very closely together and seeing kind of a lot of common actually application developers or enterprise customers were kind of -- can benefit from both technologies.
Junkai LiuLlamaIndex Inc. · President
Yes. And just I think one additional point actually, and this is just kind of like maybe a little bit orthogonal to the previous point as MongoDB isn't just a vector database, right? You can store stuff in like a key value store. You can store things like structured, semi-structured and unstructured data there. And I think one thing, especially for developers, if you're choosing like components of this like data stack to basically build applications off of you need that flexibility. And I think one thing when people like find the YouTube tutorial again to just like build some basic GenAI application, they over fit to just like really basic primitives of just like only using Vector Search in a very constrained setting.
And I think you really need a flexible storage solution, not just to store your unstructured documents, but also your operational data, figuring out how to ETL that data from like unstructured to semi-structured to structured. I think MongoDB gives you like basically a unified storage layer for all that type of stuff. And I think that's -- it's kind of like uniquely positioned to do so. There's not actually that many -- there's a lot of Vector databases, but there's not that many like solutions that provide like a unified storage solution.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Awesome. We are very proud to work with all of you who are in terms of building out this new world and the partnership. So thank you so much for joining us and sharing a bit about your story and what you're seeing in the ecosystem. I really appreciate it. Thank you.
All right. I think with that, we are transitioning back to Michael, and we're going to hear from some of our actually amazing customers. So how people actually use our technology day to day.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
All right. We're ready. Terrific. Thank you for letting us get everyone mic-ed up here. So this next section is one of my favorites over the years at this format. I always look forward to actually letting you hear directly from our customers. We obviously love to talk about our customers and our product, but there's no better substitute than actually hearing directly from them. So thank you all for joining us.
Maybe I'll just by way of introduction, like let you all introduce yourselves. We're going to try and do this as informal and interactive fashion as possible so you can get a bunch of flavor we cover as much ground as possible. So maybe just quickly introduce yourself, your organization and kind of key IT priorities, just to kind of level set with the audience. Brian, you can go ahead...
Brian Hanks
So my name is Brian Hanks. I'm the Vice President, Chief Engineer at Anywhere Real Estate. You may not be familiar with Anywhere Real Estate, but you probably are familiar with our brands. So there's Better Homes and Gardens, CENTURY 21, Coldwell Banker, Corcoran, ERA and Sotheby's as well as a title company, a relocation company, and several other entities. I guess our priorities or my priorities are to modernize the tech stack and move away from being sort of a siloed organization into more of a unified operating model. And ultimately, I'm kind of passionate about building these modern apps in a way that makes them efficient and effective, and that's where MongoDB plays a good role.
Ha Hoang
Hi, everyone. My name is Ha Hoang, and I lead Cloud Engineering Infrastructure at UKG. UKG is a workforce management software company and as a result of a merger of Kronos and Ultimate Software. So one of our kind of key priorities around modernization and resiliency. So as you can imagine, 2 large organizations coming together, there's a ton of overlap in tech and tooling. And so our modernization efforts is focused on bringing the tech stack together, especially at the suite level and a ton of modernization.
We currently have a number of technology around data -- the data layered technology, which includes Mongo. And we currently have a mix of kind of relational and nonrelational technology as well like SQL, mySQL, Elasticsearch, just to name a few. And so our focus is really around modernization and standardization.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Hjalmar?
Hjalmar Van Raemdonck
My name is Hjalmar Van Raemdonck, I'm the Vice President of ZF Group. If you do not know ZF, it is TRW in North America. So TRW is part. I see already a lot of [indiscernible], that's correct. So I'm running the digital unit at ZF. What are we doing? We are a SaaS business, and we are providing, let's say, digital tools to a fleet in order to run themselves better. So why are we doing this at ZF because we understand the braking system, the steering system, the autonomous system is so good that we turn that into as well a fleet management system. So we are mainly operating in Europe and in India, and we are starting now with our platform as well in North America.
Kevin O'Dell
My name is Kevin O'Dell. I'm a Director of Engineering at Toyota Connected North America. Toyota Connected is a Toyota company. And we're a smaller firm that really focuses on innovation. We -- our real focus is data science, AI, advanced software development, really to accelerate Toyota in the digital space. And our focus is on the connected vehicle. So all the vehicles -- or all the data that comes off of a vehicle, all the different data points and the different cameras and sensors and all that kind of stuff.
Our organization is the one that actually takes the data, the virtual assistant that's in the newer Toyota and Lexus vehicles. If you say, hey, Toyota, hey, Lexus, that response back is actually something that we built at our organization. The digital UI cockpit experience that you get from your multimedia system and your head unit of your vehicle, something that our organization designed and then the place that I actually lead Toyota Connected is, if you're ever in an accident and an agent calls into your vehicle and says, hey, are you okay? Can I help you? Can I get you 911 services? Or if you hit the SOS button in the vehicle and need some help where or your vehicle is stolen or you need roadside assistance. Any of those events that happen to you on the road, my team and my services and what we built actually does all that work or it supports all of that. And then what we're here is underneath all of that, we use Mongo as a reliable data store for us to make sure that we can respond to our customers.
Yelena Shtykel
My name is Yelena Shtykel. I'm the Head of Public Cloud Data at Citi. I'm sure all of you know, Citi, major financial firm, 200 years' history, supporting customers across investment banking, traditional banking, you name it. In terms of priorities we have is -- we are trying to take Citi the public cloud at scale. So we are focusing on all the strategic partnerships in order to be able to build those scalable ways to take the city to the public cloud. So we build in [indiscernible]. We are building self-service and automation in order to be able allow teams right at the skill of Citi to be able to leverage cloud.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Terrific. Well, again, thank you all for joining us. If we get a little bit of context of the breadth and variety. Maybe the best way to just dive in is just talk about how you're using MongoDB use cases, and we can just kind of have a conversation from there. .
Brian Hanks
Sure. I guess I'll go ahead and start.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Yes, we can mix it up. Sure.
Brian Hanks
So we'll use MongoDB primarily for our, I guess, what I would call our strategic API build. So we've been going through the process of creating enterprise capabilities and those capabilities are sometimes associated to entities, sometimes associated to actually business processes. And almost all of those capabilities are using MongoDB as their primary data store. We're also using Mongo for some of our transactional systems. So for example, we have a system called Listing Concierge. And that system allows agents and brokers to rapidly build marketing materials for listings. It uses Mongo as its operational data store. So it's using -- it uses Mongo for basically everything. And we are also beginning to branch into using Mongo for our search solutions. So our real estate and property search is powered by Mongo Atlas Search, and we're in the process of starting to look at Vector Search for some of our use cases.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Great. Who wants to jump in next?
Ha Hoang
Yes. So most of UKG's database management system uses Mongo for online transaction processing, and we use it for some data analytics as well. One of the other use cases around creating microservices. And so one of the microservices that we have migrated off of monolith is a payment service that runs payroll. And so we now have a number of other microservices that run behind it. And then specifically in our Pro product, we use it for our two -- kind of two main modules, which is recruiting and onboarding.
Hjalmar Van Raemdonck
Maybe I can add on the monolith. We are four kind of more than 10 years a customer of MongoDB. And we are more for -- more than 35 years in the space of fleet management and connectivity solutions. So we have seen the change into technology as well in the last years. And yes, like it started with the server at the fleet, and then it started with a centralized server somewhere. It was in Dublin, I think. And then we had MongoDB and we -- it was a monolith as you had as well. And we shifted 3 to 4 years ago to a new technology with microservices in the cloud. So it fits more perfectly with our culture, how we want to work. We want to make sure that the DevOp teams can own end-to-end everything.
And then we shifted together with MongoDB at the new technology of Atlas. And it's what's for us the most logic choice to shift from an on-prem to a more SaaS service. So today, MongoDB is, for us, are default for databases. It's just a default, whatever we do, every new service that we make, it is our default and especially with the new features like the time series, which is for us very important for sensor data because we are reading data from the braking system data, from the steering system and everything from every sort of sensor and this is that we are using. So MongoDB is our default in everything that we do today.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
And Hjalmar for you, I know everyone is different but for you, that's Atlas, right?
Hjalmar Van Raemdonck
Yes, correct. That's Atlas.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Maybe Kevin or Yelena?
Kevin O'Dell
Yes, I'll just say a lot of what he said very, very similar. But I will say -- so our system is fairly new. We actually insourced our -- all of our safety system, our telematics service platform. And when we first built it, starting in 2016, 2017, we actually didn't use Mongo. We used a different provider. But once we get to production, we actually ran into whole bunch of issues. And so we made a decision to convert to Mongo, which we look back on not just because I'm here, but it was a very, very good decision. We needed something that was ultra, ultra reliable, something that we really don't have to worry about. And our team is a pretty slim down team.
I don't have any real data specialists, no DBAs, nothing like that. The developers are able to use Mongo and do everything you need to do. And we've got about 200 microservices that run our platform, multiple databases underneath of that, and it's anything from your telematics data that's coming from the vehicle to say, "Hey, I'm in an accident." to our call center system about, hey, I'm talking to somebody, right, or sending messages out wherever, right?
So we've got all of that different data everywhere. And we tell our folks everything we built, it has to be up 99.99% of the time because I can't have anybody getting in a car accident and we're not there. And so we have that expectation for Mongo to always be there for our customers because it's such a critical, critical thing that we support.
Yelena Shtykel
For Citi, we've been partnering with MongoDB for many, many years, and we see growth year-over-year. Specifically at this time, I would say, like a vast majority of our usage of MongoDB is on-premises. And we are looking right now maybe the Head of Public Cloud Data. I'm obviously focusing on the Atlas quite a bit there. And it's going to be a transition, right? We are going to see some cognitive use cases where they go directly to MongoDB. Otherwise, some of them will be a migration effort from on-prem to the cloud.
And some of them will have to inevitably be on-premises, right? Because, for example, it may not make sense for them to migrate off EE version or there's a lot of complexity, right, in the legacy system to actually build it. And of course, the hybrid applications because there are certain like, Citi being like global firm, there are lots of country regulations we have to abide by. So you may not be able to move certain data to the cloud, for instance. So you wouldn't want to use MongoDB on-premises and use completely different tech stack in the public cloud. So I envision all flavors of that at Citi over time, but I would imagine with years, right, the percentage of our usage of Atlas will shift and it will increase.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Terrific. And we've touched on a couple of different parts, but maybe you can each share a little bit of views on sort of the strengths of MongoDB, sort of why you picked it. Obviously, some of it's come across in the conversation. But I think it would helpful for people to hear a little bit about your thought process. So maybe we'll start some more, Yelena, if you want to start us off, or Kevin or...
Yelena Shtykel
I would love to start, yes. Where do I start? So MongoDB, as I said, one thing that we love about MongoDB is that it's available across all major clouds. That's huge for us. Ability to have an exit strategy without spending a lot of time building a solution to actually exit and move your data to another cloud provider is huge. With MongoDB Atlas, you can just set up your cluster that spans multiple quality providers. And you can obviously do it as part of your migration, or there are certain use cases I can think of in the future where you may want to have a note that lives in another cloud provider for either business continuity reasons.
Or if you want to take advantage of some unique capability of that cloud provider and just reference the data in that cloud. So there are lots of permutations and combinations I can think of. I think that's one of the major benefits. Also the elasticity of course, being able to horizontally scale and vertically scale, that's huge for us. Security, right, of course, being highly regulated like the fact that MongoDB Atlas has [indiscernible] encryption, client-side encryption, those are key amazing features that really sets MongoDB apart.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Terrific. And then just out of curiosity, anyone else drawn to the multicloud aspect? Is that important for people?
Kevin O'Dell
I got a little something there, yes. Multicloud was important to -- I'd say almost was important to us. We actually did a big cloud migration from Azure to AWS from Toyota. And it was a year-long effort for us to do this and make sure we did it right. And we had all these steps in what we have to do. And right in the middle was we need to migrate our database, right? We're using Atlas. We're in Azure, but we needed to be in AWS.
And I will say that was the easiest and most boring part of the 4-month implementation, was clicking a button and okay, we're in AWS now. So the ability to be able to do that was just -- it was very good, but then also just the elasticity and being able to go across regions with our type of solution to where we have to be up all the time is very beneficial.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Did you have a multicloud? Yes, Brian?
Brian Hanks
We use both Azure and AWS. And so some of our applications and because we were a business that was very somewhat siloed, I guess, in the past, some of our businesses are very attached to Azure and others are -- most of our new stuff is AWS.
Hjalmar Van Raemdonck
Yes. We are not yet it in multicloud, but we are on-prem and in the cloud. So we have an old system on-prem, also MongoDB Atlas and then in the cloud as well. That's what we're using.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
And then your thoughts on strengths.
Hjalmar Van Raemdonck
On the strengths, yes.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Reasons you pick MongoDB.
Hjalmar Van Raemdonck
Apart from the feature richness that I mentioned already, one of the key things that I really like about MongoDB, and I say it more, I'm not a technical person. So I really see it more how my people are working is the customer-centricity that MongoDB has. And it's not about the customer-centricity with the focus, for example, on me because I have to pay for the invoice, it's really on the developers. They really listen to the developers, and that's what I really like. I knew MongoDB, but I know that MongoDB is already a culture in my company. It's like, "Oh, yes, we love them." That's how it is. So they were really pushing me to come over here that I really start to understand what it is.
And that's -- and you see that with other tech companies, they focus so hard on, let's say, the decision-makers, but we are having the culture that, no, no, no, we would like to have that our developers have the tools that they really believe in, and that's with us clearly MongoDB.
Ha Hoang
Yes. Similar. Our developers, I mean, they love the JSON, right, the ability to be able to store JSON directly into the database versus having to deconstruct SQL. They can also provide a rate. So we're not in multicloud yet, but it will be important for us later. But I think some other features is around, being able to have schema changes without having downtime unlike SQL. And then lastly is just the flexibility on the function calls, function call versus through SQL, which is, I think, is just another language layer that has not a ton of value.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
And you're running in a hybrid world or just the folks?
Ha Hoang
We're also hybrid, correct. So hybrid, we only got a number of our -- because we also have a number of monolithics as well. And so a lot of that is still in a hybrid and kind of in our on-prem data center. Yes.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Yes. Okay. So I'll ask all of you to put on your future prognosticating hats here, and don't worry, I read a safe harbor, so it's all fine. Don't worry about your forecasts. So just as you look ahead and you think about the ways in which you might leverage MongoDB, the ways in which you'll deploy MongoDB, some of the features of MongoDB that you sort of haven't used yet or looking to use, maybe walk me through that, what type of conversation about that. Who wants to go first?
Ha Hoang
I guess I can...
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Yes, start us off.
Ha Hoang
Yes, yes. So for us, frontline workers is a key component. And so we're certainly looking at the Vector Search as one of the key areas. And so our internal RAG team is already looking at that. And the fact that -- we like the fact that Mongo can help us scale and that we already have an existing relationship that can increase that usage for Vector Search. So that's definitely one area.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Anyone using Vector Search or any?
Brian Hanks
Proof-of-concept level.
Yelena Shtykel
Yes. Same here. There's so much -- again, like Citi is huge, so we're hearing so much need. Everybody is asking, "When are you going to make it available?" Because at our organization, in order to make a capability available, we have to go through certain governance process. So there is just so much interest in Vector Search capabilities for sure. That's what I was going to say as well.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Kevin or Hjalmar?
Hjalmar Van Raemdonck
Yes. I'm not sure if you use it, but I think we use it. I'm not that close to the development, but I'm pretty sure.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
And if you think about things when you look forward, what's your outlook?
Hjalmar Van Raemdonck
We are discovering the Edge as well. Today, you have the time series, you have the geo index. It's all because we are so focused on the data in the vehicle, and our goal is to do much more with the vehicle. That's our goal with the mixed fleet. If it's coming from Toyota or from somewhere else, it doesn't matter for us. We are really in the commercial vehicle space. That's where we are. And we believe so strongly in the Edge as well. And that's something we are going to discover.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
And Kevin, when you look out into the future, what do you see?
Kevin O'Dell
We actually got -- some of our developers actually investigating some of the time series, some of the geo capabilities as well. Obviously, when you start thinking out a couple of weeks, you get into the AI type of talk, right? So definitely top of mind for us, everything with automotive.
Brian Hanks
Yes. We're going to continue expanding Mongo for basically everything we build new. We're using a lot of Atlas Search, and that will continue to expand. But the vector piece is the big piece for us. We have about 3 or 4 different proof-of-concept level things that we're working on right now. And I hope that we'll have some stuff around natural language, property search, enhanced property suggestions so that it's easier to actually get a potential buyer attached to the right property.
And ultimately, I'm even looking at vector from the perspective of it doesn't matter what type of application it is, whether it's a music app or a social media app or whatever. There's -- there are the apps that stand out from others, right? And if you look at those apps, they're generally -- there's something about them. There's a level of intuition that you get from using the app that you wouldn't get from a typical search. So when I think of Vector Search, it goes beyond search, it's how do I start building an app that actually helps the user learn something that they wouldn't have learned otherwise?
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Terrific. And then maybe just to bring us home here, we'll touch on AI. It would seem like I would not be doing my job if I didn't at least raise the topic, although we're going to exempt Yelena given Citi's compliance requirements. But maybe we can just talk a little bit about kind of AI strategy, how you're thinking about things, what AI-powered applications you're looking for? And so how do you envision things unfolding over both the near and the long term, recognizing that it's early on. So yes, Hjalmar, start us off?
Hjalmar Van Raemdonck
Yes. I can start. Maybe it's good that I explain which problem we are trying to solve.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Yes. Great context.
Hjalmar Van Raemdonck
So we provide solutions to the fleet. And the fleet has an amount of vehicles necessary, it has to go to A, to B. It has to stop over there and all the rest. And in the office, there are dispatchers, and they have to make decisions the whole time. Dispatcher, and I'm giving now the context of Europe, can handle depending how complex it is on heavy commercial vehicle fleet, 8 to 12 assets. And that's kind of the limit of a dispatch to do it very long.
And we are thinking about, hey, we want to scale this. We don't want to have that the dispatch is becoming a limited factor in operations of a complete fleet. We have a fleet of 8,000 trucks, 10,000 trucks, but also of 10 and 20 trucks. So what do we do is we provide a solution to optimize that one, but we also provide a solution to understand if the vehicle is still healthy. But the dispatch is getting so much data life, it's getting sales that you sometimes does not know what to do. And this is exactly what you're trying to discover with MongoDB is not only historical data, this is about the machine learning models, but more on the live data.
Having the live data, all the notifications, everything, all the alarms that are coming, having an AI model so it can start to assist the dispatcher. And that's what we're doing and trying to build step-by-step in such a way that we can scale a dispatcher and at a certain moment, automate everything. That's what we're trying with AI.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Great. [ Go from there ]. Yes, Kevin, go for it.
Kevin O'Dell
I can jump in, yes. So if you think about from a Toyota perspective, you can start all way from manufacturing and things you can do while you're manufacturing a car with a -- from an AI perspective, even if it's just fixing the machines that break when they're making a car, right? So there's things that we're doing and we're talking about like how do we automate that? Or how do we use AI to help with that, all the way up to the dealer and helping the dealers get leads and communicate with their customers.
But then where we really come in is after the sale or after the purchase of the vehicle and how do we use AI in a customer-first way, make the -- owning a Toyota or Lexus more enjoyable for the customer. And so there's things that we talk about there. We've already got an automated assistant in the vehicle, but how can we make it smarter? How can we make it more predictable about what the consumer or what the driver wants to do?
In the safety space, something that we are tangibly doing right now is we've got about 300 safety agents that are waiting for someone to call them. They've always got to be ready. And we get calls for subscription or kidnappings, suicides, accidents, like wide range of calls that we have to get. And so trying to use AI to figure out how do we help our agents in those situations, whether it's someone just asking about a subscription or someone saying that, "Hey, my mother with dementia just took my vehicle." How can we assist our agents using our past experience and our knowledge bases to proactively help them right away or if they're talking to law enforcement?
There's a lot of stuff you can do there in a call center environment. So we're tangibly looking at that. Obviously, Mongo plays a piece in that. We have a lot of our data, our call recordings, all that kind of stuff is all accessible with Mongo. So being able to keep that stuff close, it's something that's -- it's essentially on our road map as we're talking.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Great. Before I go to -- Yelena, I don't know if there's anything you want to add personally from -- maybe not from a Citi strategy standpoint, but just use your own views on AI or I can jump to how -- whatever is best.
Yelena Shtykel
Yes. I mean I will just say that what I can say is that there is a huge focus of Citi on AI, right, because we see that as something that will help our transformation, and we want to be at the forefront of technology. So there's a lot of focus. We are looking at the AI as something they can help the developers, not replace the developers, but actually help them to be more productive. We are looking at developer doing quite a bit because that's the low-hanging fruit.
But of course, there are lots of use cases across the board that are being identified and needless to say, there's a lot of interest for sure. But we have to do it in a careful, controlled way. So that's a big area of focus.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Perfect. Thank you. Ha?
Ha Hoang
Yes. So for UKG, it's -- our focus is frontline workers. And so how do we make them more productive, whether it's through scheduling, reporting. And so we're planning to embed GenAI into a lot of the end user-facing features to get them to be more productive.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Great. Brian, you mentioned the POC.
Brian Hanks
Actually, I have stuff in production already.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Okay. Walk us through those.
Brian Hanks
I have 3 different use cases in production. I have what I'll call a summarization use case, where we have a relocation business, and there's multiple types of communications that come in over phone calls and text messages and emails. And being able to take all that information and send it into a model and generate a summary of report and send it out to the person who's relocating, that was a multi-hour process. Now it's down to minutes. We also have, within our marketing and advertising product, we're using AI models to write listing descriptions, and we are actually scrolling up today. We have added it.
Yes. We've rolled it out another feature today, and this feature is even more exciting to me because what we're doing is we're using an Anthropic model. and we're feeding, listing photos into the model, and we're generating captions and detailed attributes, like the level of attribution on the photos is such that it can enable search. Like instead of just being like that's a front door, it will tell you that's a metal front door with a grate. It's a very, very detailed attribution. And so those are already in production.
We're also looking at and/or POC-ing multiple other things, including how do we use AI models to be able to answer real estate domain-specific questions for agents and brokers or even potential homebuyers. We're using AI models right now to enhance our leads so that you get a better chance of matching a lead to an agent properly. And I can go on and on about AI, but there are numerous different use cases, including what I mentioned about vector earlier, right, using the different vector-embedding model so that you can actually enhance the results set.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Terrific. Thank you all for that. Thank you for being customers. Thank you for being here. Thank you for taking the time to share with this audience. I really appreciate it. Thanks.
Brian Hanks
Thanks.
Yelena Shtykel
Thank you.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
All right. Next up, I think we have a partner discussion. And so with that, we will give us a minute transition, and we'll be right back.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Okay. I think there's few people trickling in, but we might as well get started. So thanks again for being here. This section is all about partners, and it's my privilege to have Alvaro Celis with us today from Microsoft. I'll let Alvaro explain his role and scope of responsibilities. But obviously, partnering is a key part of our strategy in terms of our business. You heard us announce the MAP Program, and we're very pleased to have Microsoft be part of that program. And I would tell you that as Alvaro has told me that he doesn't do this for every partner. So very grateful that he came out here.
And obviously, as you imagine, there's a high bar to do these kinds of things at Microsoft. So we're very appreciative of his time. And he did take a red eye, so he's wired with a lot of coffee. So -- but nevertheless, we're grateful to have you here today. Thank you very much. So just to start off, maybe just for the group to understand, if you could just explain your role and scope of responsibility at Microsoft.
Alvaro Celis
Thank you, Dev. By the way, thank you so much for the kind invite and the kind introduction. It's a pleasure to be here and it's a distinction. I've been with the company for a very long time. I'm going to disclose that, if you promise not to do any math, 32 years will be in September. So it's been a lifetime of adventures and work across the company. Started in Latin America, still in Asia being the VP of the region out of Singapore and the last 8 years in global roles.
In the current -- my current position for the last 2.5 years where I've been partner with your team, is we work on global ISVs. And the mission of the team is to be sure that we look across our commercial solution areas. We look at the long-range plan. We look at where the opportunity is for us to accelerate and enhance that plan, partnering with ISVs, strategize our coverage and who do we need to work with, and we have the opportunity and the esteemed pleasure to lead global relationships. That's where we are a team day in and day out.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Thank you. And before I get into specifics of the Microsoft-MongoDB relationship, is there any -- you just touched on a little bit about the strategy. Was there any maybe details you can provide on how you think about who to partner with, when you partner, when you don't?
Alvaro Celis
Yes, that's a great question. And at the heart, we are a platform company. So the philosophy is quite simple. You co-innovate and you share success. So we believe that when you provide partners with great platforms, tools that are world class, you have the appropriate level of support and you nurture them in an ecosystem that is in the same approach, you accelerate innovation, you create more customer value. Then you complement that with a GTM that plays to the strength of that partner, allow them to be more successful to create more differentiation, of course, and to accelerate AI data transformation across industries because that's what people want.
Look, at the end, it's around enabling customer choice, it's being sure that if you're a customer, you have the best-of-breed solutions in that platform. We are very clear when we have this vibrant ecosystem, we can work with more customers, and we can work with them even in more scenarios and more use cases. So it's absolutely a win-win where the partners win, the customer win and we win.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
So the -- clearly, the follow-on question, I think, will be is what's the appeal with partnering with MongoDB?
Alvaro Celis
That makes sense. Okay. When you look at MongoDB, think about the role of the developer, right, and how critical they are in driving all this innovation, and the strong preference that developers all over the world have for MongoDB. So for us, it's about serving our shared customers and being sure that we can make MongoDB Atlas a world-class, a best-in-class experience for the customers in Azure, our shared customers.
Then you go on and say, "Well, there's a [ fine pace ] of acceleration of customers moving into the cloud." So how can we help them make the most out of that? How do we secure that Azure is the best platform for those MongoDB workload or if a customer wants to develop a cloud-native MongoDB application, and they can do that in Azure the best way possible. In this area of AI, accelerating the migration and having the data in Azure will help customers realize more value. So what's not to love?
On that creation of value cycle, you and the team, honestly, have done an incredible work creating value around the database. When you look at search, stream processing, Edge and all the work you are doing to be sure that the data management is more robust. So we can use that as part of the innovation that we do together to generate more value for customers that are going into the cloud in this sort of AI.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Great. So we spend a lot of time, we get a lot of questions from people here and other investors about how do you specifically work with the cloud partners? And now we have lots of common customers: Boots, Temenos, Toyota, et cetera, who are all very -- obviously, customers of both of ours. Maybe you can be a little bit specific about how we work together on customer engagement, your point of view because obviously, we talked about a lot from our point of view.
Alvaro Celis
Yes, that's great. Well, MongoDB is a global ISV, which we have a very close relationship. You have a priority there of partners. So it's a very deep relationship between both companies, and you know that. So maybe the best way to explain that, think about maybe 3 layers. Let's talk about the product, let's talk about integrations, let's talk about the go to market, okay?
So when you look at the product, our engineering teams are working together to be sure that MongoDB Atlas experience on Azure is best-in-class. Actually, the teams are committed to be sure that our shared customers have that world-class experience. But in the last few months, as you know, we have lighten up more than -- a total of 48 Azure regions where we have now MongoDB Atlas available for more Azure customers, and that has been fantastic, as an example of the co-engineering work that your team and ours are doing together.
Then when you look at integrations, it's about the work on these data and search capabilities that Mongo has and you look at Azure data services and how we are creating synergies that allows for our shared customers to make the most out of that synergies across our product lines like Fabric, Power BI, Azure data services and so on.
And then number three, our teams are really working together in the GTM across sales, marketing and delivery, being sure that we keep being guided by customer choice and the customer preference. One thing that you and I were talking before coming in here is how much our cultures found themselves with synergies in the customer obsession. I wanted to do what is right for the customers, and learning that when we work together, when MongoDB Atlas is the right solution for the customers, we own leasing credit value for those customers. And that has become a fuel and an inspiration on the partnership.
Sometimes the Mongo team calls Microsoft, sometimes Microsoft call the Mongo team. We always are respecting customer choice. We have a clear communication guidelines that allow us to be sure that we're always on the same page on what is the customer choice that we're enabling. And that formula is resonating greatly.
Yes, over the last 6 months, to give you an idea, 6, 7 months, MongoDB has been one of our top Azure Marketplace partners, right? So that's a cool testament of customer preference.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Yes. I would -- this follows a good segue to our next question is I would say it would be -- it would not have been likely for you to be on the stage 2 years ago, like our relationship was a little bit more fragile and things have changed a lot. How would you describe the current state of our relationship?
Alvaro Celis
I think it's a long-standing partnership, as you mentioned. But it's a partnership that actually last year, when we signed a new agreement and we reimagined the relationship, we took it to the next level, right? And we just talked about getting in progress. That has been fantastic. The customer experience now today on MongoDB Atlas and Azure is world-class and is getting better and better. And we are delighted to hear that.
Our teams are doing -- are really guided by that customer choice. That is allowing us to work in service of the customer in a very mature way, which is a completely different level of impact on the one that we have. We are actively working in that with the curiosity and passion for the customer, what -- now that we have landed and doing this for you, what else can we do? And that is unlocking more value, allowing us to learn where do we work together to generate more impact for customers. And as the technology evolves, we're also seeing new opportunities to bring synergies across the Mongo stack and the Microsoft stack in service of those customers, right?
So it's a continuum and we see the flywheel going in a very, very nice way. We have made way more than a year of progress in the last 12 months. Thank you for that, by the way.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Thank you. Another question that comes up often is when we launched Atlas, there were some investor skepticism about could we both partner and compete with the hyperscalers, not just with you but all the hyperscalers. And so you obviously have a broad portfolio of first-party services. How do you -- how does Microsoft and the Azure team think about partnering versus competing when it comes to working with partners?
Alvaro Celis
So that's a very good question, and it goes back to my opening on, if you're a platform company, that's a core part of what you need to embrace and make happen because it's about customer choice. You want the customers to find the depth and the breadth of options in your platform that will suit for the needs that they have and the suits are very varied, very broad.
Your partners extend, complement, specialize and create incremental value on your platform. When you present both options, you are just making the customer more successful and your partner more successful. In other words, this is not new. As you know, I mean, we have that in many, many fronts, and I think that we have the maturity and the practices that allow us to have very clear rules of engagement, very constructive discussions on how do you make the platform better, how do we make your offering better in service of those customers.
And on the execution on the GTM side, as I mentioned, our teams have matured a ton on those rules of engagements, communication, and always being guided by customer choice, which ended up being the best North Star for any relationship in the industry.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Yes. And we have -- as you mentioned, we have integrations on your first-party console. You have co-seller arrangements when you start a program. There's a whole bunch of things that we do together.
Alvaro Celis
Exactly.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Obviously, you can't go to a tech conference without talking about AI.
Alvaro Celis
It took to do that long.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
So Microsoft is obviously really been prescient about obviously making some very strategic investments in OpenAI and leading the industry in AI. How is MongoDB and Microsoft working on AI?
Alvaro Celis
Okay. No, that's a very, very fair question. Can I take 2 minutes to give the audience answers on what are we doing?
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Please, please.
Alvaro Celis
So when you think about it, we all know that AI is the most consequential technology of our times. So how do we democratize the impact that they can bring so you can empower more people, more companies so they can be more productive and they can go and tackle the biggest opportunities and challenges of the generation, right? That's kind of where the heart is and is tied to our mission as a company.
Now our approach, when you look at, again, being a platform company is depth and breadth of offerings on AI. So it's a combination of our own research, partnership, investments that allow us to have a whole breadth across the stack of AI to offer customers solutions that might fit in multiple scenarios, in different needs, in different locations, depending on the strategy, depending on the solution.
Now what is very clear, Dev, and that's where we start shaping up our partnership is, data is the power that fuels AI. And if you're a customer that is trying to get -- make the most out of the AI capabilities, you need a world-class data estate in the cloud, well managed, well supported, well maintained. And that's where our opportunity with partner companies like MongoDB in service of the customer to have that data estate in the cloud are incredible.
And we partner at multiple levels when you look at our approach of partnerings in AI. Sometimes we partner at scale, trying to support with our Copilots, thousands or hundreds of people to be able to go and achieve potential and productivity through those solutions. And in some other cases, we create highly sophisticated, specialized, bespoke solution that are AI powered. That is what the business needs, right?
In that continuum, we have multiple level of partnerships. Our aspiration is to be sure that Azure is the best place for your data as a customer and offering the broadest range of options for that data, database relation and the relationship of the source, scripting, whatever you have in there and where partner offerings as we're talking, right? So that's where MongoDB Atlas become part of that offering.
I think that the ultimate layer where we are working and getting a lot of traction lately is how do we partner to help customers accelerate solving those problems and getting time to value. I mean when our customers are starting to work on generative AI, there's a lot of enthusiasm and aspiration on what you can do. But in reality, there's friction, there's complexity and sometimes, you don't have even the whole talent. So what we can do together to make it simpler for them to have integration, synergies in our stack is going to be -- is going to make a huge difference.
An example is the work that we have done with the Semantic Kernel and your MongoDB Atlas Vector Search, and how those two things combined will help customers that are MongoDB Atlas financial customers, being able to streamline data management and have semantic queries that will fuel an input to your specialized AI applications, right? So that's the type of partnership that we have today, and we are -- we keep exploring more opportunities moving forward.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Got it. And last question before we wrap is, again, with all the safe harbor caveats, how do you see this relationship going forward?
Alvaro Celis
I think that the relationship will keep growing in strength. I think that we are now in that positive spiral where the success that we're having, the customer momentum is fueling confidence on each other, confidence on what we're doing and giving us permission to be more curious and more options to work on that. I think on the -- for example, on the migration path, we are working now on taking approach that will really streamline and accelerate customer migration with the Migration Factory approach that we're doing together.
So we can help the customers have that time to value and time to migration in a completely different level with quality, uncertainty, right? I think that our work on the selling side, customers keep pitching us where it's joint value. So we keep listening to them. That opportunity is going to be there for us to find newer scenarios that when we work together, we create more value for them. And as I was mentioning, the whole synergy on our technologies and that passion to innovate together in service of the customers, one of the things that you will see more and more progress. And I know that there's good news comment on the road that we will share with the audience when the time is right.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Yes. Very excited. Well, Alvaro, thank you very much for being here. We're very grateful for the partnership, and we'll look to do great things together.
Alvaro Celis
Great to be here. Thank you everyone.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Thank you.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Thank you for that. We're going to run through the last agenda item, which is really a business update before we get to Q&A, where Sahir and Dev will join me, and we'll take any questions that you all have. So just on the business side, there are three things I'm going to cover. I'm going to provide a bit of a market and kind of product-level update. We'll talk on this continuing theme of the benefits of becoming a standard, and I'll give you a little bit of a financial slice of what that looks like. And then lastly, conclude with the financial summary overall.
So with that, as I mentioned at the very beginning, this very large market, $94 billion growing to $153 billion. But we're still in the early innings of enterprise. You heard one of the panelists earlier today referenced over 0.5 billion downloads. Clearly, plenty of developer adoption, but we're still working on penetrating in enterprises. So we have about 3/4 of the Fortune 100, about 1/2 of the Fortune 500 and about 30% of the Global 2000. So we still have a lot of landing to do in that land-and-expand model.
But even within these accounts, we also have a fair amount of expanding to do within the ones we've landed. So we have relatively small market share. This is based off of the IDC data, and we had them cut this a couple of different ways. So the left-hand side shows you the 2.4% estimated market share that we have among Fortune 100 customers and roughly similar, 2.1% among Fortune 500 customers, so the customer base is broadly diversified.
We talked about how we have to win workload by workload. We talked about the benefits of becoming a standard within so you can kind of increase that market share. But what this shows is that we still have a long runway for growth not only in landing new accounts, as I mentioned kind of with this slide previously, but also in expanding within our existing accounts. And so that's really what sets up this market opportunity for us and that we've talked about.
So I'll spend a minute talking about products, so let's dig in. Atlas is obviously the fundamental driver of growth. And given how large the market is growing, we've had a lot of success, but we only have about 2.6% share within the cloud database market. So again, kind of just to give you a relative sense of there's a huge amount of opportunity here still for us to go. I want to talk about Atlas and Atlas consumption in particular.
For those of you who pay attention and were here last year or watched, this slide was very well received and helpful view of the world, so we thought we'd revisit it and give you an update on how all these things stand. I can see by the cameras and iPhones everything going up that, that is true again. And so what we have here is now a 12-quarter view, right? And this shows week-over-week average consumption. So we talk about trends. This is what we're thinking about. This is what we're looking at. And what you can see is those first 5 quarters, we're sort of in the kind of the pre-macro view of the world. And the last 7 quarters are more sort of macro affected and sort of in the slower growth range.
Again, what we're talking about here is still growth. And so what you could see is there is seasonal variability. But in general, these last 7 quarters when we talk about stability, this is what we mean. Now we've told you before, we don't have a ton of data points when it comes to seasonality, but we're calling out the patterns that we've seen. We're obviously still learning about seasonality. But in general, you can see that Q1 and Q3 tend to be stronger than Q2 and Q4. We also talked about last year that we were seeing more stability, and also that stability was translating into less variability.
So here, if you look at the bars and you focus on Q3, you can see that Q3 this year wasn't as strong as Q3 last year. Conversely, Q4 this year was stronger than Q4 of last year. And so that's sort of that reduced variability of outcomes that we've talked about. So that's a quick update on Atlas consumption, but we thought that it would be helpful to revisit it, given the interest and attention and the value that you all found in that previously.
We could say that for the Q&A, but the answer is going to be no. So -- we thought it would be helpful to spend a little bit more time on this, though, and give a deeper view to explain what is stable consumption look like when you look at it in the workload level, right? Part of what we've talked about is sort of this winning workload by workload. And so let's put that in context because ultimately, I think it's most helpful to think about consumption as -- making up the aggregate consumption of all the sort of underlying workloads, right? So I'm now going to present something that is highly illustrative, ignore seasonality and is not to scale.
So this is not the slide to take out your protractor and ruler and everything to assess what's going on. But hopefully, this will help you all understand what we're talking about when we're describing the business. And so let's dig in. Consumption growth obviously begins with the existing workloads that we already have on the platform. And those workloads grow over time. But naturally, as they age, their growth rate does slow. And so what you'd see is if you didn't add any new workloads, you wouldn't have stable consumption. Your growth rate, our growth rate would naturally slow over time, right as those workloads mature.
And so if you -- sorry, I just have to click ahead here. And so as we acquire new workloads, they add to the growth. And as we've said before, those new workloads grow more quickly initially. So what you can see here is if we add in the green, you'll see the impact of new workloads that we add in the first 4 quarters, right? So this is just an 8-quarter view. And what you can see is as you add in those first workloads, they sort of are accretive to growth, right? They're adding to the growth rate. And then subsequently, the blue lines, which I'll sort of fill in here are the impact of new workloads that you add in quarters 5 through 8.
And so in aggregate, again, we're ignoring seasonality, this is sort of illustrative. But what you can see here is the result of the stable growth is a result of a mix of workloads, existing workloads, new workloads and everything else. And so theoretically, if you take that kind of theoretical construct, and you apply it practically, what that meant in fiscal '24 was the workloads that we acquired during the year in addition to those that we acquired in the year before, just to keep this 8-quarter view consistent, contributed to the stable growth that we saw in fiscal '24 relative to fiscal '23.
So hopefully, that helps paint a little bit more understanding around Atlas consumption and also sort of tries to tie it into this workload level view that we've been walking you through. Switching over to EA. As we said before, EA continues to succeed and has outperformed our expectations. Part of that is the success of the running [indiscernible] strategy. Part of that is also the fact that it's a huge market. And even though we've had great success and offered EA longer, we actually only have an estimated 1.1% of the on-prem market.
And so I think that sort of helps further explain the success and durability that we've seen within EA. Many of the EA customers are also still quite early in their cloud journeys. So this takes a look at the EA ARR, that 83% of the EA ARRs for customers who are 80-plus percent EA. So again, we're starting to see people adopt more cloud workloads. I expect we'll continue to see that trend and that evolution. But many customers are still quite early in their cloud journeys.
Lastly, just for completeness, we'll talk about sort of the other. Just we've called this out. It's particularly important as it relates to our guidance and some of the numbers we've previously shared with you, fiscal '24 was an unusually strong quarter year -- excuse me, for this sort of non-EA non-Atlas portion, including as a result of multiyear, we mentioned Alibaba and others.
We referred to this in our guidance, but hopefully, this kind of visually helps people understand why that creates a headwind in fiscal '25. So that's a bit on sort of market and products. I'll spend a minute talking about the benefits of becoming a standard. And it's important to understand, this is really not about the size of how their current level of spend. It's really more about the size of the future opportunity.
And so typically, even a customer who are spending a lot, who might be a digital native wouldn't necessarily be a great category to think about standardization. And so we talked about standardization early. I want to walk through the benefits of being a strategic account that we call we've talk to you about this program where -- and these are accounts where we're already a standard or close to being a standard, but where there's a lot of opportunity left. So again, back to like the digital native concept. If they pick MongoDB, they've built their whole business on MongoDB or coreplication of MongoDB, we're going to benefit from their growth. We don't need to think of them as a strategic count that we're trying to incrementally drive our wallet share from kind of low single digits into much higher.
So here's a snapshot of the strategic accounts program, distribution mix by of ARR by product on the left-hand side. So a fair amount still in EA, which isn't surprising. These tend to be largest customers, customers who have huge IT spends where there's still room to run. The middle bucket shows industry slightly skewed relative to the bulk of the business.
We have a very diversified broad business. But within this strategic accounts program, disproportionate representation from financial services in part just because those have been -- customers who have been early on the platform, and there's just a lot of running room within those accounts.
Obviously, there's some overlap interplay between the EA side and the financial services side, as you might expect. And then lastly, more skewed to the Americas, again, just because that's where some of the older, longer relationships are. One of the reasons that the strategic accounts program has been successful is because we've been quite disciplined about it. And we've learned more information over time as we've continued to iterate. So these are some of the examples of the conditions that need to be present in an account. We need to have adoption from multiple developer teams. We need strong, not just technical chain, but strong business champions.
We need to make sure that we're demonstrating momentum in the account and that there's a clear pipeline of opportunities for us. And so the kind of account would be right for the incremental investment. What does that incremental investment look like? You take the rep and they're solely focused on the account. They also take a higher quota in order to help us recoup and generate a good return on the incremental investments we're making. They get additional investments from presales from sales development reps from SDR, some customer success. Field marketing, developer relations, things like that.
And then there are additional resources that are available to them sort of on a case-by-case basis where as we invest more of their quota goes up even further. So it's not really just about the incremental investments, so, it's about finding the right time when this account is ready. The result is that these extra resources cost more money, right? Not surprisingly, if you think about sort of a selling unit as a standard unit, they cost about twice as much as the average direct selling unit because we're putting these incremental investments into them. But what we see is about 7x more new workload ARR being generated in a given year from these accounts.
So part of that might say, well, why don't you just do more of these? It's not just because of the investment. It's not just because you suddenly call an account or strategic account that you get 7x out for putting 2x in. It's about the discipline, it's about the timing. It's about being ripe and ready for that investment. And that's a lot of what you've heard about throughout the whole day and really throughout the quarters as we talk about these things. What we're doing on the product side, what we're doing on the go-to-market side, what we're doing to get these accounts in a better position to move more and more accounts to the status because we'd love to do that.
So the result of all this is that even though these accounts are quite larger or meaningfully larger. They actually are growing much faster than the accounts. And so that's really been a terrific addition to the results. So that's just a little bit of a view of kind of the financial lens of like what becoming a standard means and why it's something that we continue to focus on.
Lastly, in terms of financial summary, we've gone through significant growth as a company capitalizing on our market opportunity. We've also made material progress in terms of our operating margins on a non-GAAP basis. The balance, as we've talked about with many of you, the balance of our margin progress relative to our market share gains has been a little bit lopsided. So that's what explains the sort of 10 to the 16 last year, going to 10% at the midpoint of our estimates for the current fiscal year, but that still represents about 500 basis points of progress over the 2-year period.
I'll spend a minute just dissecting and disaggregating what the margin movement looks like from the 16 to the 10, just so people understand. There are really 2 key drivers. The first is the impact of the $80 million of onetime high-margin revenue that we've talked about. That's from the excess multiyears that we had last year as well as the unused credits and commitments. And then the second piece is from hiring. We had our hiring last year. It was extremely back-end weighted. And so this year, what we're doing is we're hitting the run rate anniversary or the run rate or the annualization of those, and that's worth about another 200 basis points, which leads to the 10%.
Just quickly on the hiring to give you a little bit more of the details there. Last year, we slowed hiring from kind of 30-plus percent to 9%. But of that 9% in the first half of the year was only 1%, right? So you can see that the growth rate was abnormally back-end loaded. And so that's what creates the sort of 200 basis points headwind that we talked about previously.
Part of the reason that it was back-end loaded and we talked about this a call or 2 ago, think a couple of calls ago, was that fiscal '24 was more stable than we had expected, which is great. And obviously, that benefited our results, but we weren't sure.
And at the beginning of the year, we contemplated a range of outcomes, some of which were bleaker than others. And so we probably waited a little bit too long to add heads. I think we've known that throughout the year, that it was going to be as stable as what would have been, we would have been more aggressive in adding heads. And so that's part of what results in the back end nature of the headcount.
We are still investing both in sales and marketing, given the limited footprint that we have as well as in R&D, and you heard a number of announcements, and we'll continue to do that. And so I just want to reiterate our long-term targets at 70-plus percent on the gross margin, 20% plus on the non-GAAP operating margin. Obviously, 16% last year and even 10% projected for this year, we've made a huge amount of progress since our IPO where we were sort of in the negative mid-30s. So we've made 45 to 50 points --45 points or 50 points of our 55 points that we needed to make, but we're still only about 2% market share.
So again, we're trying to make sure that balance is responsible for long-term investors and to capitalize on the opportunity in the long run. I'll spend a minute just on history of free cash flow here. So despite this reduced emphasis on upfront commitments, last year was the first year that we generated significant free cash flow. As we've said before, we've been focusing on reducing friction in the sales process that obviously has cash flow implications. So historically, we've been in the range of about 100% to 110% of collections as a percent of revenue. You can kind of see that over the prior 4 years. It's a little bit volatile, it depends on the timing of payments, but generally, that's like the range that we're in.
In fiscal '24, we further reduced the emphasis and the incentives for upfront commitments. And what you can see is that percent of revenue -- collections to the percent of revenue declined to 92% and we'll continue seeing that in general in fiscal '25, and that's sort of reflected in our internal view. The result of that is that we have less upfront cash and correspondingly, that affects the free cash flow dynamic. What this tries to do is just tries to look at free cash flow, less operating income as a percent of revenue to try and put that cash flow dynamic, that cash cycle into context.
We found it not as helpful to look at an absolute basis, just given the growth of the business. So this tries to put it on a percent basis. So you can see it was sort of a source, if you will, early on. But over these last couple of years as we've been making this multiyear journey to reduce friction and everything else, that has implications when you're thinking about cash flow and things like that.
Lastly, as executives and shareholders ourselves, we do care about dilution and are very sensitive to equity dilution. We primarily think about dilution on a net share basis, so as you can see, we've been focusing on reducing dilution. Obviously, there are a couple of financing events that convert in fiscal '20 and secondary offering, secondary primary offering in fiscal '22. Excluding those over the last couple of years, we've been bringing burn down -- dilution down from just about 6% in fiscal '20 to about 2.5% this past year.
So this is something that we pay close attention to. Our Board does as well. And from all the data that we have from our comp consultants and everything else, we feel like we're very much in line with the benchmarks, but it's something we continue to monitor closely and pay attention to.
And so with that, I will invite Dev and Sahir up here, and we will take any questions. Question mic, Serge and Brian, thank you.

Question and Answer
Unknown Analyst
So I guess the first question I wanted to ask to you, Dev, is obviously a lot of focus on GenAI, kind of we heard from some partners and customers that this is kind of a new category of data, a new category of apps. I guess as we think about the history of MongoDB when the nonrelational or no SQL market kind of unfolded, Mongo wasn't actually the #1 player at the get-go. You had companies like MarkLogic or Couchbase or whatever, obviously, you've evolved and become that biggest player. But I guess how do you kind of see the share dynamics contrasting in the GenAI world relative to the NoSQL wave. In other words, are you winning kind of those high-value use cases and workloads today? Or is this going to be something that kind of needs the industry to mature and these apps to go more mainstream for you to win the lion's share?
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Yes, I'll start, and welcome Sahir and Michael to add any comments. What I said in the last earnings call, and I kind of implied it today is that I think we're still in the very early innings of AI. I think the bulk of the investments are being made more in the infrastructure layer. When we talk to customers, customers over the past year have been doing a lot of experiments, but there's not that many applications in production. So first point is that we're still early.
What we've been working on is to make sure that we're well positioned to be the platform of choice for people to build applications on and we're doing that on a couple of dimensions, one, relying on our strengths and being able to handle multiple data structures. I talked a little bit about in the keynote about we are well versed to support different kind of data structures, which is really critical in AI, when you're trying to process voice, video, images, et cetera; two, being very open and flexible because, obviously the space is evolving quickly. We see customers who may experiment with one used case, one set of technologies and then maybe the second experiment is used in something else.
So we're LLM independent, cloud independent. We work with a bunch of other application frameworks. So we're really trying to make sure customers have choice and make it very easy and the third one is that we're trying to, obviously, and that's part of our DDP strategy is trying to address the broader set of used cases because we've heard it clearly from customers that they can't really manage and have 17 different players of databases or platforms to support. And so that's essentially our strategy.
We feel pretty good about our position, just by the nature of the number of early-stage companies who are building AI apps already on top of MongoDB, not that we take that for granted. It's early. It's not those companies that started to break out and become large companies like we saw. I think that's going to take time, like we saw in the cloud where people like coin based started early and became big companies. Good to hear. Started early and became a big company, a relatively big company.
So I think the shakeout -- it's unclear what models what use cases and all that will really take off. But the fact that so many are building on MongoDB makes us feel good. And I think as we talked about earlier today is we've seen VectorSource become very popular. We've seen a bunch of people who want to be part -- I mean, there's not many companies at the risk of sounding -- having little hubris who have AWS, Google and Microsoft who are all partnering with us, right? And that speaks to how popular MongoDB is on all their clouds. And so from that point of view, I think we're well positioned, but it's definitely early. I don't know if you want to add anything.
Sahir AzamMongoDB, Inc. · Chief Product Officer
I think the only other piece I'll add is one of the things we are encouraged by, and actually, we mentioned this AI event, I don't know, 3 weeks ago or whatever in London, one of the chief product officers were one of the big model companies. One of the things you said that kind of stuck with me and that we're certainly seeing is this moves AI out of the corner office of the team working in data science to now becoming something that's mainstream for every development team to have to figure out.
And so in many ways, yes, AI generally, as a category has been around for a long time, machine learning approaches, et cetera. But it's always been sort of very niche and focused on a part of the organization that's kind of around understanding insights as opposed to really driving customer experience or driving true business process efficiency and that's bringing AI now right to where we think our wheelhouse is. And that's really driving what a lot of these innovative start-ups and partners that we're working with because they see us that's having that developer love.
Sanjit Kumar SinghMorgan Stanley, Research Division · Vice President
To pick up on Sahir's point around and in your point, Dev, around the sort of era right now of experimentation. Do you wonder if that's actually an inhibitor to your growth? Because I would imagine if we were coming out of a multiyear or multi-quarter downturn in sort of tech spend, and we're coming out of this, has there not been this Gen AI wave or Vectors have probably been modernizing applications, building net new applications.
Do you think there's a sense that as we focus on these POCs and the Evals that sort of taking energy away from developers in that would, in some way, inhibit in the near-term MongoDB's growth? or I just want trying to get a sense of the dynamic cycle that we're...
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
To make sure I understand question, if I can paraphrase what you said, just to -- so there's a finite amount of development capacity in any organization. And if you're saying if some of the development capacity is being siphoned off to do these AI experiments, does that mean that there's less "mainstream" apps being built today. I think that's a very reasonable thing to assume. Obviously, it depends on the customer, but I think this is such a long-term play. This is such a massive opportunity.
We're quite excited about the long-term opportunity here. And I think all the building blocks are coming together. And I think we are seeing customers at one level, I see customers overwhelmed with in terms of what's the rate and pace of change, but also see them fearful because they're always paranoid by one of their competitors using AI as a competitive advantage to disrupt them. And so there's this tension where I want to be thoughtful about what I do, but I also want to get going. And so -- we're kind of seeing that with our customers.
Sanjit Kumar SinghMorgan Stanley, Research Division · Vice President
If I could just follow up. I think the message that I got from all the content today is that Mongo is going to go prosecute the opportunity. You're just not going to let the opportunity to come to you. You guys are going to be actively involved. There is a point that you made on like pushing reference architectures. We saw like the last decade of cloud, we experimented with OpenStack, OpenShift, right, Swarm, Docker and then we ultimately landed on Microservices Kubernetes, Edge stack.
How influential do you think you guys can be in terms of helping customers land on a reference architecture that can create time to value? And any sense of like -- is that going to take 2 years, 3 years, 5 years? Any perspective?
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Yes, I wish I could give you a clear forecast on how long it's going to take. The one thing that has actually I would say, surprised us, but it makes sense is that customers do view us as a credible thought partner. One, we know how to build modern apps. Two, we're not talking up our own book because it was not like we have our own LLM or we have some partiality to any one cloud. And so customers actually end or by the way, we run on-premise. And that's something that we're starting to hear more and more customers who are saying I like the notion of the fact that I can run these AI apps on-premise, right? Not everything is going to go to the cloud.
And so that -- so that gives us, I think, a reasonable voice when talking to customers about what to do where to go. I mean this event that Sahir and I were at in London, I mean the quality of customers who came was spectacular, not just from the U.K. but all over Europe. And they generally want to learn and listen and they also want to learn from their peers. And there was, in some ways, palpable relief in the room because everyone felt like they were behind, but then they realize everyone is kind of the same boat as them. They're all in this experimental phase, and they always worry that someone's got to figure out and they're just going hog wild in terms of rolling out a set of apps.
And I think -- the short answer to your question is, I think we can be quite a valuable thought partner to our customers.
Raimo LenschowBarclays Bank PLC, Research Division · MD & Analyst
Raimo Lenschow from Barclays. Can you talk a little bit about the migration opportunity? Like conceptually, I can see how you can kind of make it a lot easier. But the question is like how easy can it be. Like if you look at GenAI now or things like "Oh my God, it's going to do it all for you. It's probably not going to be that easy, like, but how far do you think you can put in? And how easy do you -- can you think you can make it for someone to migrate because there's obviously a clear kind of platform migration opportunity out there.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Yes. I think it's going to be a journey. And that's the mindset and philosophy we've taken on it. It is like we were clearly enamored by the idea that these tools and technologies can be used to expand from kind of the data migration layer, which is where we were kind of focused on with relation to migrator originally, but really now apply to the hardest part, which is the application itself and all that kind of legacy code. So I think we clearly had an instinct and interest and all the demos we've been seeing about how this makes developers more efficient, there's a clear kind of link there.
I'll say just personally, I was more on the skeptical side at the beginning of the journey, and people like Paul, who you heard from earlier, we kind of started a program and said, let's go try this app because if it was impactful, even 10%, 20%, 30% improvement widens the amount of applications for which the business case makes sense in significant -- in terms of even on per account basis.
So that was kind of the drive to -- and Dev really pushed I think, listen, we got to try something really research, this is going to be really impactful if it works. Well, let's go learn. And so that was kind of the mindset. Clearly, as you heard from Paul, there's a fair amount of iteration and manual effort in each one of those pieces of the process. So it's not like autopilot by any definition. It's absolutely an assistant and an iterative process. But I would say the results are better than we expected. The more skeptics in the room like myself are actually now more like let's go even further on this. And then in the time period over the last few months that we've been working on these pilot projects around app modernization, the model quality and race continues as well.
So if you project out 2, 3 years, not just the individual model quality, but then these agentic workflows that may be able to automate more of the process that right now is manual, we do think the mix between manual effort and automation will only continue to improve, but we do see it being a journey. The idea of like a push button migration of complex application that's housing live mission-critical applications, I don't think making that automated completely is going to be something that's going to happen anytime soon.
Brad Robert RebackStifel, Nicolaus & Company, Incorporated, Research Division · MD & Senior Equity Research Analyst
Brad Reback from Stifel. Dev, you had mentioned earlier today when you were talking to the Microsoft ISV rep, the relationship was fragile a couple of years ago, it's doing better now. I get the sense it's still lagging maybe where AWS and GCP are. So if you think about the opportunity for that to be a growth driver over the next few years, could you size it up for us?
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
I actually think it's actually better than you described. It's really become quite -- the relationship tenure has changed. I think -- what I would say is the GCP is always easy because they didn't have competitive products.
With Amazon, it was easy to orient around customers, they're very customer-obsessed. I think with Microsoft, they were kind of feeling their way through what I think got them over the hump was, one, they saw how popular we were, both in their cloud and just in the industry overall. And two, they saw the consumption that we were driving in their cloud themselves. And they realize it's not a zero-sum game. It's actually a win-win for both parties. And we've had some big wins with them. And a big part of that, that we focus on -- a lot of people think it's all about product and product integration.
A big part of this is all about the go-to-market angle. Like it's -- I and Scott Guthrie can align on great -- have a handshake, but if the sales people are not incentivized and they can't see how it's going to affect their pocket book, they're not going to want to really work together. So I saw Alan here. I guess you may have a [indiscernible] out for another. Yes. Alan has our partnerships. He and his team do make a lot of investments in making sure that the go-to-market teams are highly incentivized to work together. You actually also remember, we're a member of the first-party counsel, we're member the start-up program. So we're trying to also remove a lot of the friction. We're in the co-sell program, so all the friction around working together, if you can chip away, it just becomes that much easier to work together.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Just one additional kind of anecdote I was talking about yesterday actually. And one of the things that's interesting is kind of prior especially, I think, before this big boom around everyone's focus on AI, a lot of start-up developers were focused on Google or AWS, I know Microsoft certainly has its own ecosystem to smaller.
Now in the start-up program performance, we're seeing actually a shift here now where Azure in the start-up program for more the smaller earlier stage company is actually driving higher volume on Azure than we've seen in years prior working with them.
Rishi Nitya JaluriaRBC Capital Markets, Research Division · Analyst
Rishi Jaluria from RBC. Maybe just a 2-parter on GenAI. First, when we think about tools like GitHub CoPilot, Tabnine, right. They're causing 40%, 50% greater productivity. In fact, one of your partners said 10x to 100x. Has that resulted in any sort of increase in the rate and velocity of applications being built on Mongo. Have you seen any of that? How do you think about the longer-term opportunity? And maybe just alongside that, how do you think about the opportunity to use a lot of these GenAI technologies to create kind of a MongoDB CoPilot, right, especially given there's probably not nearly enough MongoDB developers out there for the level of demand. That would be really helpful.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
Do you want to take that one?
Sahir AzamMongoDB, Inc. · Chief Product Officer
Yes, absolutely. So I do think over the long term, the volume of software that will be developed in the world will go up. Like just naturally, if it's easier for developers to be more efficient to create applications. You're already seeing some simple app type frameworks automatically generate kind of components of applications, that's only going to continue.
So over the long term, I definitely think that will be a trend. I think it's too early to like be able to measure that and the impact in terms of volume of apps specifically, but I think it's a pretty obvious direction over the span of years.
I think in terms of Copilot, this is an interesting one. We did look really at like do we have to go train or build our own model for MongoDB and what we found is, actually, no, there's a lot of great examples in documentation, all that publicly available, not just from us, but you look at all the content that's been created like go to YouTube and search learn Mongo. There's a whole bunch of third-party stuff. There's folks that run Mongo courses in Brazil. They get tens of thousands of people signing up digitally.
So we already saw the baseline where these Microsoft or AWS or GCP, et cetera. The baseline of quality was quite good. But then we said, okay, how do we actually make sure that we can raise the bar given they're all partners of ours. And so we actually work with those providers directly, gave them some of our more proprietary information of best practices, our own code samples, to be able to make all those models kind of more effective. And that's something we're going to continue doing, not just with them, but any of the other assistance.
And in reverse, obviously, we've implemented some of those public LLM into our own developer experience. So it's a nice kind of circular relationship. And we've seen really good results so far without finding the need to have to create our own LLM to do that versus kind of using this fine-tuning approach.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
By the way, on the stats on the developer productivity improvements, it's all over the math depending on who you talk to. I was talking to the CIO of one of the largest banks in the world, and he said that they see about 20% to 25% developer productivity, but they're only coding for half the time, right? So that means it's about 10% to 12.5% in terms of real productivity. While that's not -- nothing to sneeze at, I think what you hear from customers is literally all of the map in terms of what these Code Gen tools can do.
Brent Alan BracelinPiper Sandler & Co., Research Division · MD & Senior Research Analyst
Brent Bracelin from Piper Sandler. I wanted to stick with this AI trend with the topic here around timing and then consumption opportunity. On the timing side, maybe I'll ask a question a little differently for you, Michael. Consensus is modeling a trough in your growth rate at about 11% in Q2, and then it has it picking up in the second half of the year.
How much of that is just tied to year-over-year compares? Or is it concurrent with the assumption that you'd start to see these experiments in AI go into production, start contributing to the growth profile, one. And then two, Sahir for you, as you think about a cloud workload versus an AI workload. How different is it? Branded anomic was mentioning an exponential increase in data and volumes. Walk me through what you're seeing in the same experimentation area around the consumption differences for AI versus cloud?
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
So I'm happy to start. As I said at the very beginning, we're not going to talk about the recent quarter, obviously, because it only closed 2 days ago. But in our March call, when we gave our guidance, I think we were really quite clear that our view was AI -- hugely beneficial long-term trend not going to show up in the near-term numbers.
So I'll let consensus speak for itself in terms of whatever consensus things. And I don't know what embedded assumptions they have, but at least our internal assumptions are -- it's -- we're going to be a long-term beneficiary and it's a long-term tailwind, but it's not something that's going to reduce the numbers in the short term.
Sahir AzamMongoDB, Inc. · Chief Product Officer
Yes. And I think it makes it particularly hard to kind of tease apart right now because a lot of those applications we mentioned are early, they're in POCs, there it's an experimentation. So by definition, the consumption of the AI workloads, if you look at them, are probably smaller than the production cloud workloads that have obviously had many years to mature or serving live customers.
So that kind of makes it hard to get an apples-to-apples comparison. But in a more abstract way, if you think about these workloads, what [indiscernible] VPs like to say, an AI application is still an application. So you still have operation transactions, you still have search, you still have stream processing, all these various needs.
So that's kind of a baseline and then you now have the compute and storage needs of the vectorization and management of the AI data. So there could be a world over time as these apps mature where an AI workload could on a unit basis be larger than an average application, but it's way too early to tell because of this kind of experimentation versus production dynamic difference between the two.
Michael Joseph CikosNeedham & Company, LLC, Research Division · Senior Analyst
Mike Cikos with Needham over here. A question for you on the strategic accounts. I know that you called out the 2x upfront investment and the 7x faster growth trying to get a better sense of how are you guys measuring success on this go forward, right? The thought process being the first potential to cherry pick the strongest accounts first. That's the ones that you went after, right, versus, I guess, fine-tuning the learnings that you continue to take on. So is that 2 to 7 expected to hold like should that deteriorate over time? How are you thinking about that?
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Yes. So a couple of different thoughts. So in general, because of the attractive setup that we just walked through, if you think about if you are fly on the wall in a budgeting conversation, right? The first thing you should want to do is fund as many of those as you can, right? And that's sort of where you start. But back to my earlier comments, just because we call them strategic accounts, isn't what actually makes them successful, right? It's sort of all the other things that go around it. And so we tend to be very returns-oriented and disciplined. And so I don't think we would suddenly just throw a whole bunch of people in the program on the hopes that it would work out. And so what we've done is each year as we've grown the program, we said, okay, what's the next set of accounts that's ready. And with the goal of everything that we're doing to be have more accounts to be ready for that, right? and be kind of primed for that investment, if you will.
And so I wouldn't overly fixate on sort of like the 2x and the 7X and that those are immutable and we'll always do those theoretically. If you had confidence that you could double the pool, and it would only be 2x and 6x, you'd probably still do that, right? But like what we're doing is we're tackling or taking advantage of as many as we can and trying to get more ready.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
If I can just add one other comment would be that one of the takeaways also for us is that unlike other businesses who use sales and marketing tools to do a lot of pipeline generation, one of the things that we've learned through this effort is that a big part of the pipeline generation for us is developer education because a lot of people -- it's amazing to us, even some of the accounts that Michael showed where we're doing meaningful revenue, there's still developers in those accounts who just don't know MongoDB that well.
So consequently, when you don't know MongoDB that well, you may not choose MongoDB to address the next new workload you're being asked to focus on. And so developer education is like, in some ways, for us, a pipeline generation point of view because once they realize how easy to use to MongoDB to solve a particular, say, content generation or e-commerce or payments application and to use MongoDB to do that, it just unlocks so much opportunity. So we are going to be, over time, feathering in more technical resources relative to sales resources to -- in these larger accounts, maybe not to the ratios of these strategic accounts to really help with that developer education and awareness.
And much like this conference here, where we have a lot of technical people and customers come and learn about products and all that. The reason we're doing this in 23 locations around the world, and that doesn't include all the things we do inside accounts where we have hacker days, and developer days, it's all about that developer education and enablement because that's where the unlock comes in for us.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
And okay, look, we have time for at least one more -- time for one more if this is just last one because I know we are out of time, but go ahead, Jason, please.
Jason Noah AderWilliam Blair & Company L.L.C., Research Division · Partner & Co-Group Head of Technology, Media and Communications
Jason Ader with William Blair. Thanks for a very informative session, guys. I have one clarification and one question. The clarification is, is there only one database standard in the strategic account or there tend to be 2, 3, 4, 5?
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
It really depends on the account because depending on large account, they may have a modern standard and a kind of a legacy standard just because there's some apps that they know they're not going to -- not necessarily move or if they make any additional incremental investments for legacy platforms, they'll stay on one standard. So that typically, depending on the account, we kind of see a modern standard and legacy standard and then an exception process for everything else.
Jason Noah AderWilliam Blair & Company L.L.C., Research Division · Partner & Co-Group Head of Technology, Media and Communications
And then the question is just on the AI subject. It feels like over the next 3 to 5 years, there's 3 main opportunities for you with AI and tell me if I'm wrong, but vector search consumption, leveraging Gen AI for relational migrator and then just this idea of more code being generated that you referenced, that's going to generate more apps. How would you rank order those 3 opportunities, let's call it, 3-year time frame in terms of the impact on your business?
Sahir AzamMongoDB, Inc. · Chief Product Officer
That's a tough question. I would say, certainly, the vector database adoption is one of the soonest things we're seeing. So the big question mark there is just how fast it will take -- how long it will take for those applications to become real production applications that are successful on and that's on our customers, actually not us.
But as Dev alluded to earlier, our strategy there is to cast as wide in that as possible. So it become a standard if it's a large enterprise and adopted vector database or if it's work through all our start-up programs, one of these types of events to get the broad base bottoms up kind of start-up community. I think I'd probably say that's probably the most near-ish term thing, but it's dependent on the apps.
App modernization, obviously, those are bigger, more established applications that on a per workload bases have higher dollars, but this is very much in the early stages of us applying AI to that problem set. So encouraging. But I think before it's not just having some great pilots, it's how does that turn to product? How does that turn into a repeatable process? How do we enable the skills of our services organizations to not do a couple of pilots, but actually get this to repeatable motion, and that naturally takes some time.
I think if I had to guess the compounding of more software on a global scale being generated is probably going to be the furthest out, but perhaps in time, may have the biggest long-term impact, but this is just speculation. I think it's really hard to put.
Dev C. IttycheriaMongoDB, Inc. · President, CEO & Director
I'm long term, I'm pretty bullish about the throughput of development teams. The point I would make is that if you look at what happened in the industrial era, right, how people make cars, how people create a clothing. It's very much a bespoke process, right? And then the industrial revolution kind of standardizes and created that whole manufacturing process to do things at scale. If you look at software development, software development is still very much a bespoke developer-by-developer kind of process. And I think what AI will do long term is really allow people to produce code at a massive scale. Again, I'm talking longer term, not necessarily in the short term. And that, I think, is going to fundamentally change the amount of software that's produced in our industry.
Michael Lawrence GordonMongoDB, Inc. · COO & CFO
Maybe just simplistically, I would say if you they are all tailwinds, they're all significant. And even though we're 6. 5 years after our IPO, they would all be excellent parts of a slide that said multiple vectors for future growth, if you were putting your IPO roadshow slide together.
And so with that, thank you for joining us. Thanks for spending the afternoon with us and hope it was valuable.